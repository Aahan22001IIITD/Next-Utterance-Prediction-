{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11407762,"sourceType":"datasetVersion","datasetId":7145939}],"dockerImageVersionId":31013,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate nltk bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:59:30.110661Z","iopub.execute_input":"2025-04-14T21:59:30.111511Z","iopub.status.idle":"2025-04-14T21:59:42.597780Z","shell.execute_reply.started":"2025-04-14T21:59:30.111470Z","shell.execute_reply":"2025-04-14T21:59:42.593293Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from evaluate) (2.0.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/site-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from evaluate) (24.2)\nCollecting multiprocess\n  Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.10/site-packages (from evaluate) (0.3.9)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from evaluate) (2.32.3)\nCollecting xxhash\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting datasets>=2.0.0\n  Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/site-packages (from evaluate) (2025.3.2)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from evaluate) (4.67.1)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk) (8.1.8)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk) (2024.11.6)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/site-packages (from bert_score) (2.6.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/site-packages (from bert_score) (3.10.1)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/site-packages (from bert_score) (4.51.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting aiohttp\n  Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting multiprocess\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec[http]>=2021.05.0\n  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nCollecting dill\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (0.6.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.5.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.21.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib->bert_score) (4.57.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib->bert_score) (3.2.3)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.8)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<6.0,>=4.0\n  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.0/334.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nInstalling collected packages: xxhash, propcache, nltk, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets, bert_score, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.9\n    Uninstalling dill-0.3.9:\n      Successfully uninstalled dill-0.3.9\nSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 async-timeout-5.0.1 bert_score-0.3.13 datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.4.3 multiprocess-0.70.16 nltk-3.9.1 propcache-0.3.1 xxhash-3.5.0 yarl-1.19.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport nltk\nimport numpy as np\nfrom datasets import Dataset, DatasetDict, load_from_disk# load_metric might be deprecated, use evaluate\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n    EarlyStoppingCallback\n)\nimport evaluate # Use HF Evaluate library\nimport logging\nimport tempfile # Import tempfile\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:59:42.600357Z","iopub.execute_input":"2025-04-14T21:59:42.600610Z","iopub.status.idle":"2025-04-14T22:00:45.229852Z","shell.execute_reply.started":"2025-04-14T21:59:42.600585Z","shell.execute_reply":"2025-04-14T22:00:45.221557Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1744668030.145655      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:230\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"MODEL_NAME = \"t5-base\"\nDATASET_PATH = \"/kaggle/input/endsem-eval-nlp\" # <--- Use the correct Kaggle path\nOUTPUT_DIR = \"/kaggle/working/t5_base_counseling_finetuned\" # <-- Save output to /kaggle/working/\nMAX_INPUT_LENGTH = 512\nMAX_TARGET_LENGTH = 128\nBATCH_SIZE = 8\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 4\nWEIGHT_DECAY = 0.01\nLOGGING_STEPS = 100\nEVAL_SAVE_STEPS = 400\nEARLY_STOPPING_PATIENCE = 3\nEARLY_STOPPING_THRESHOLD = 0.001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:45.232784Z","iopub.execute_input":"2025-04-14T22:00:45.233315Z","iopub.status.idle":"2025-04-14T22:00:45.243703Z","shell.execute_reply.started":"2025-04-14T22:00:45.233287Z","shell.execute_reply":"2025-04-14T22:00:45.239329Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import logging\n\n# Clear existing logging handlers (important in Jupyter)\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)\n\n# Now reconfigure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler()]\n)\n\nlogging.info(\"Logging is now properly configured and working in Jupyter!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:45.245009Z","iopub.execute_input":"2025-04-14T22:00:45.245239Z","iopub.status.idle":"2025-04-14T22:00:45.850468Z","shell.execute_reply.started":"2025-04-14T22:00:45.245213Z","shell.execute_reply":"2025-04-14T22:00:45.845394Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:45,839 - INFO - Logging is now properly configured and working in Jupyter!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# *** --- START OF FIX --- ***\n# Define writable cache and temp directories within /kaggle/working/\nkaggle_working_dir = \"/kaggle/working/\"\nhf_cache_dir = os.path.join(kaggle_working_dir, \"hf_cache\")\ntemp_dir = os.path.join(kaggle_working_dir, \"tmp\")\n\n# Create directories if they don't exist\nos.makedirs(hf_cache_dir, exist_ok=True)\nos.makedirs(temp_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:45.853302Z","iopub.execute_input":"2025-04-14T22:00:45.853601Z","iopub.status.idle":"2025-04-14T22:00:45.872785Z","shell.execute_reply.started":"2025-04-14T22:00:45.853575Z","shell.execute_reply":"2025-04-14T22:00:45.866884Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Set environment variables for Hugging Face libraries\nos.environ['HF_HOME'] = hf_cache_dir\nos.environ['HF_DATASETS_CACHE'] = os.path.join(hf_cache_dir, 'datasets')\nos.environ['TRANSFORMERS_CACHE'] = os.path.join(hf_cache_dir, 'transformers')\nos.environ['TMPDIR'] = temp_dir  # For underlying tempfile usage\nos.environ['TEMP'] = temp_dir   # Windows compatibility (though not needed on Kaggle)\nos.environ['TMP'] = temp_dir    # Common variant\n\n# Explicitly set tempfile directory (optional but can help ensure consistency)\ntempfile.tempdir = temp_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:45.874820Z","iopub.execute_input":"2025-04-14T22:00:45.875040Z","iopub.status.idle":"2025-04-14T22:00:45.898251Z","shell.execute_reply.started":"2025-04-14T22:00:45.875018Z","shell.execute_reply":"2025-04-14T22:00:45.892079Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"logging.info(f\"Set HF_HOME: {os.environ.get('HF_HOME')}\")\nlogging.info(f\"Set HF_DATASETS_CACHE: {os.environ.get('HF_DATASETS_CACHE')}\")\nlogging.info(f\"Set TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE')}\")\nlogging.info(f\"Set TMPDIR: {os.environ.get('TMPDIR')}\")\nlogging.info(f\"Set tempfile.tempdir: {tempfile.tempdir}\")\n# *** --- END OF FIX --- ***","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:45.898998Z","iopub.execute_input":"2025-04-14T22:00:45.899209Z","iopub.status.idle":"2025-04-14T22:00:45.920980Z","shell.execute_reply.started":"2025-04-14T22:00:45.899188Z","shell.execute_reply":"2025-04-14T22:00:45.917152Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:45,908 - INFO - Set HF_HOME: /kaggle/working/hf_cache\n2025-04-14 22:00:45,909 - INFO - Set HF_DATASETS_CACHE: /kaggle/working/hf_cache/datasets\n2025-04-14 22:00:45,910 - INFO - Set TRANSFORMERS_CACHE: /kaggle/working/hf_cache/transformers\n2025-04-14 22:00:45,911 - INFO - Set TMPDIR: /kaggle/working/tmp\n2025-04-14 22:00:45,912 - INFO - Set tempfile.tempdir: /kaggle/working/tmp\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogging.info(f\"Using device: {device}\")\nif device.type == 'cuda':\n    logging.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:45.924234Z","iopub.execute_input":"2025-04-14T22:00:45.924956Z","iopub.status.idle":"2025-04-14T22:00:45.942737Z","shell.execute_reply.started":"2025-04-14T22:00:45.924928Z","shell.execute_reply":"2025-04-14T22:00:45.936533Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:45,931 - INFO - Using device: cpu\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# --- 1. Load Dataset ---\nlogging.info(\"Loading dataset...\")\ntry:\n    train_dataset = Dataset.from_file(\"/kaggle/input/nlp-test/val.arrow\")\n    val_dataset = Dataset.from_file(\"/kaggle/input/nlp-test/train.arrow\")\n    test_dataset = Dataset.from_file(\"/kaggle/input/nlp-test/test.arrow\")\n\n    raw_datasets = DatasetDict({\n        'train': train_dataset,\n        'validation': val_dataset,\n        'test': test_dataset\n    })\n    logging.info(\"Datasets loaded successfully:\")\n    logging.info(raw_datasets)\nexcept Exception as e:\n    logging.error(f\"Error loading dataset from {DATASET_PATH}: {e}\", exc_info=True)\n    exit()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:45.944472Z","iopub.execute_input":"2025-04-14T22:00:45.944981Z","iopub.status.idle":"2025-04-14T22:00:46.024400Z","shell.execute_reply.started":"2025-04-14T22:00:45.944937Z","shell.execute_reply":"2025-04-14T22:00:46.018366Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:45,948 - INFO - Loading dataset...\n2025-04-14 22:00:46,011 - INFO - Datasets loaded successfully:\n2025-04-14 22:00:46,013 - INFO - DatasetDict({\n    train: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 4008\n    })\n    validation: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 576\n    })\n    test: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 968\n    })\n})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# --- 2. Load Tokenizer ---\nlogging.info(f\"Loading tokenizer for {MODEL_NAME}...\")\n# (Tokenizer loading code remains the same)\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nexcept Exception as e:\n    logging.error(f\"Error loading tokenizer {MODEL_NAME}: {e}\", exc_info=True)\n    exit()\n\nprefix = \"generate response: \"\nlogging.info(f\"Using prefix: '{prefix}'\")\n\n\n# --- 3. Preprocessing ---\nlogging.info(\"Setting up preprocessing...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:46.026202Z","iopub.execute_input":"2025-04-14T22:00:46.026467Z","iopub.status.idle":"2025-04-14T22:00:48.186083Z","shell.execute_reply.started":"2025-04-14T22:00:46.026437Z","shell.execute_reply":"2025-04-14T22:00:48.180481Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:46,028 - INFO - Loading tokenizer for t5-base...\n2025-04-14 22:00:48,174 - INFO - Using prefix: 'generate response: '\n2025-04-14 22:00:48,176 - INFO - Setting up preprocessing...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- 3. Preprocessing ---\nlogging.info(\"Setting up preprocessing...\")\n# (preprocess_function remains the same)\ndef preprocess_function(examples):\n    # ... (keep the function definition as before) ...\n    try:\n        inputs = [prefix + str(doc) for doc in examples[\"input_text\"]] # Ensure input is string\n        model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n\n        # Set up the tokenizer for targets\n        targets = [str(text) for text in examples[\"target_text\"]] # Ensure target is string\n        labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n    except Exception as e:\n        logging.error(f\"Error during preprocessing batch: {e}\", exc_info=True)\n        try:\n            logging.error(f\"First input item: {examples.get('input_text', ['N/A'])[0]}\")\n            logging.error(f\"First target item: {examples.get('target_text', ['N/A'])[0]}\")\n        except:\n            logging.error(\"Could not retrieve specific items from batch during error.\")\n        raise e\n\nlogging.info(\"Applying preprocessing to datasets...\")\ntry:\n    # *** START OF MODIFICATION ***\n    # Define explicit cache file names in the writable directory\n    writable_cache_base = os.environ.get('HF_DATASETS_CACHE', '/kaggle/working/hf_cache/datasets') # Use env var if set\n    os.makedirs(writable_cache_base, exist_ok=True) # Ensure base cache dir exists\n\n    # Create a dictionary mapping split names to full cache file paths\n    cache_file_names = {\n        k: os.path.join(writable_cache_base, f\"{k}_tokenized_{MODEL_NAME.replace('/','_')}.arrow\")\n        for k in raw_datasets.keys()\n    }\n    logging.info(f\"Using explicit cache files: {cache_file_names}\")\n\n    tokenized_datasets = raw_datasets.map(\n        preprocess_function,\n        batched=True,\n        num_proc=os.cpu_count() // 2 or 1,\n        cache_file_names=cache_file_names, # <--- Explicitly provide cache file paths\n        load_from_cache_file=True # Default is True, but being explicit can help\n    )\n    # *** END OF MODIFICATION ***\n\n    logging.info(\"Preprocessing complete.\")\n    logging.info(tokenized_datasets)\nexcept Exception as e:\n     logging.error(f\"Error during dataset mapping (preprocessing) even with explicit cache files: {e}\", exc_info=True)\n     # Log environment variables again to double-check\n     logging.error(f\"Current HF_HOME: {os.environ.get('HF_HOME')}\")\n     logging.error(f\"Current HF_DATASETS_CACHE: {os.environ.get('HF_DATASETS_CACHE')}\")\n     logging.error(f\"Current TMPDIR: {os.environ.get('TMPDIR')}\")\n     exit()\n\n\n# Remove columns not needed by the model\ntry:\n    tokenized_datasets = tokenized_datasets.remove_columns(raw_datasets[\"train\"].column_names)\n    logging.info(\"Removed original text columns.\")\nexcept Exception as e:\n    logging.warning(f\"Could not remove columns (maybe already done or preprocessing failed?): {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:48.187997Z","iopub.execute_input":"2025-04-14T22:00:48.188243Z","iopub.status.idle":"2025-04-14T22:00:54.322093Z","shell.execute_reply.started":"2025-04-14T22:00:48.188220Z","shell.execute_reply":"2025-04-14T22:00:54.316364Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:48,196 - INFO - Setting up preprocessing...\n2025-04-14 22:00:48,197 - INFO - Applying preprocessing to datasets...\n2025-04-14 22:00:48,199 - INFO - Using explicit cache files: {'train': '/kaggle/working/hf_cache/datasets/train_tokenized_t5-base.arrow', 'validation': '/kaggle/working/hf_cache/datasets/validation_tokenized_t5-base.arrow', 'test': '/kaggle/working/hf_cache/datasets/test_tokenized_t5-base.arrow'}\nMap (num_proc=48): 100%|██████████| 4008/4008 [00:01<00:00, 3544.43 examples/s]\nMap (num_proc=48): 100%|██████████| 576/576 [00:00<00:00, 1281.26 examples/s]\nMap (num_proc=48): 100%|██████████| 968/968 [00:00<00:00, 2036.36 examples/s]\n2025-04-14 22:00:54,264 - INFO - Preprocessing complete.\n2025-04-14 22:00:54,266 - INFO - DatasetDict({\n    train: Dataset({\n        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 4008\n    })\n    validation: Dataset({\n        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 576\n    })\n    test: Dataset({\n        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 968\n    })\n})\n2025-04-14 22:00:54,311 - INFO - Removed original text columns.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:54.323037Z","iopub.execute_input":"2025-04-14T22:00:54.323268Z","iopub.status.idle":"2025-04-14T22:00:54.333263Z","shell.execute_reply.started":"2025-04-14T22:00:54.323241Z","shell.execute_reply":"2025-04-14T22:00:54.328846Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"logging.info(f\"Loading model {MODEL_NAME}...\")\ntry:\n    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n    model.to(device) # Move model to GPU if available\n    logging.info(f\"Model {MODEL_NAME} loaded successfully to {device}.\")\nexcept Exception as e:\n    logging.error(f\"Error loading model {MODEL_NAME}: {e}\", exc_info=True)\n    logging.error(\"Check model name, internet connection, and available memory (especially GPU RAM).\")\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:54.334758Z","iopub.execute_input":"2025-04-14T22:00:54.335117Z","iopub.status.idle":"2025-04-14T22:00:57.828742Z","shell.execute_reply.started":"2025-04-14T22:00:54.335095Z","shell.execute_reply":"2025-04-14T22:00:57.822705Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:54,340 - INFO - Loading model t5-base...\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n2025-04-14 22:00:54,635 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n2025-04-14 22:00:57,817 - INFO - Model t5-base loaded successfully to cpu.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# --- 5. Metrics ---\nlogging.info(\"Setting up metrics (BLEU and BERTScore)...\")\ntry:\n    bleu_metric = evaluate.load(\"bleu\")\n    bertscore_metric = evaluate.load(\"bertscore\")\nexcept Exception as e:\n    logging.error(f\"Failed to load metrics using 'evaluate' library: {e}\", exc_info=True)\n    logging.error(\"Make sure 'evaluate', 'bert_score', and 'nltk' are installed: pip install evaluate bert_score nltk\")\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:57.830594Z","iopub.execute_input":"2025-04-14T22:00:57.830862Z","iopub.status.idle":"2025-04-14T22:00:59.846738Z","shell.execute_reply.started":"2025-04-14T22:00:57.830826Z","shell.execute_reply":"2025-04-14T22:00:59.840252Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:57,832 - INFO - Setting up metrics (BLEU and BERTScore)...\nDownloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 15.0MB/s]\nDownloading extra modules: 4.07kB [00:00, 4.91MB/s]                   \nDownloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 13.0MB/s]\nDownloading builder script: 100%|██████████| 7.95k/7.95k [00:00<00:00, 8.39MB/s]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# --- 5. Metrics ---\nlogging.info(\"Setting up metrics (BLEU and BERTScore)...\")\ntry:\n    bleu_metric = evaluate.load(\"bleu\")\n    bertscore_metric = evaluate.load(\"bertscore\")\nexcept Exception as e:\n    logging.error(f\"Failed to load metrics using 'evaluate' library: {e}\", exc_info=True)\n    logging.error(\"Make sure 'evaluate', 'bert_score', and 'nltk' are installed: pip install evaluate bert_score nltk\")\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:00:59.848711Z","iopub.execute_input":"2025-04-14T22:00:59.848979Z","iopub.status.idle":"2025-04-14T22:01:01.172478Z","shell.execute_reply.started":"2025-04-14T22:00:59.848949Z","shell.execute_reply":"2025-04-14T22:01:01.165728Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:00:59,850 - INFO - Setting up metrics (BLEU and BERTScore)...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# NLTK is needed for BLEU tokenization\ntry:\n    nltk.data.find('tokenizers/punkt')\n    logging.info(\"NLTK punkt tokenizer found.\")\nexcept nltk.downloader.DownloadError:\n    logging.info(\"Downloading nltk punkt tokenizer...\")\n    try:\n        nltk.download('punkt', quiet=True)\n        logging.info(\"NLTK punkt tokenizer downloaded successfully.\")\n    except Exception as e:\n        logging.error(f\"Failed to download nltk punkt: {e}\", exc_info=True)\n        logging.error(\"Check internet connection or manually download.\")\n        exit()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:01:01.174356Z","iopub.execute_input":"2025-04-14T22:01:01.174608Z","iopub.status.idle":"2025-04-14T22:01:05.021061Z","shell.execute_reply.started":"2025-04-14T22:01:01.174582Z","shell.execute_reply":"2025-04-14T22:01:05.017080Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLTK punkt tokenizer found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/local/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizers/punkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLTK punkt tokenizer found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDownloadError\u001b[49m:\n\u001b[1;32m      6\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading nltk punkt tokenizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","\u001b[0;31mAttributeError\u001b[0m: module 'nltk.downloader' has no attribute 'DownloadError'"],"ename":"AttributeError","evalue":"module 'nltk.downloader' has no attribute 'DownloadError'","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')  # Download the tokenizer\nnltk.data.find('tokenizers/punkt')\nlogging.info(\"NLTK punkt tokenizer found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:01:11.104549Z","iopub.execute_input":"2025-04-14T22:01:11.104961Z","iopub.status.idle":"2025-04-14T22:01:11.583061Z","shell.execute_reply.started":"2025-04-14T22:01:11.104928Z","shell.execute_reply":"2025-04-14T22:01:11.576644Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n2025-04-14 22:01:11,571 - INFO - NLTK punkt tokenizer found.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n\n    if isinstance(predictions, tuple): # Sometimes Trainer wraps predictions\n        predictions = predictions[0]\n\n    # Decode generated tokens into text\n    # Replace -100 (ignore index) with pad_token_id for decoding\n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n    try:\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    except Exception as e:\n        logging.error(f\"Error during token decoding in compute_metrics: {e}\", exc_info=True)\n        # Log shapes to help debug\n        logging.error(f\"Predictions shape: {predictions.shape if isinstance(predictions, np.ndarray) else 'N/A'}\")\n        logging.error(f\"Labels shape: {labels.shape if isinstance(labels, np.ndarray) else 'N/A'}\")\n        # Return dummy metrics to avoid crashing training, but log the error clearly\n        return {\"bleu\": 0.0, \"bertscore_f1\": 0.0}\n\n\n    # Simple post-processing: remove leading/trailing spaces\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [label.strip() for label in decoded_labels]\n    # Handle empty predictions/labels which can cause errors in metrics\n    decoded_preds = [pred if pred else \"<empty>\" for pred in decoded_preds]\n    decoded_labels = [label if label else \"<empty>\" for label in decoded_labels]\n\n    # Compute BLEU\n    try:\n        tokenized_preds_for_bleu = [nltk.word_tokenize(pred.lower()) for pred in decoded_preds]\n        tokenized_labels_for_bleu = [[nltk.word_tokenize(label.lower())] for label in decoded_labels] # BLEU expects list of references\n        bleu_result = bleu_metric.compute(predictions=tokenized_preds_for_bleu, references=tokenized_labels_for_bleu)\n        if bleu_result is None or \"bleu\" not in bleu_result:\n            logging.warning(\"BLEU computation returned None or unexpected format. Setting BLEU to 0.\")\n            bleu_score = 0.0\n        else:\n            bleu_score = bleu_result[\"bleu\"]\n    except Exception as e:\n        logging.error(f\"Error computing BLEU score: {e}\", exc_info=True)\n        bleu_score = 0.0 # Assign default value on error\n\n    # Compute BERTScore\n    try:\n        # Specify language for BERTScore model (important)\n        # Also specify the model type if default isn't optimal (check BERTScore docs)\n        bertscore_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\", device=device) # Run on same device as model if possible\n        if bertscore_result is None or \"f1\" not in bertscore_result or not bertscore_result[\"f1\"]:\n            logging.warning(\"BERTScore computation returned None, empty F1 list, or unexpected format. Setting BERTScore F1 to 0.\")\n            bertscore_f1 = 0.0\n            bertscore_precision = 0.0\n            bertscore_recall = 0.0\n        else:\n            bertscore_f1 = np.mean(bertscore_result[\"f1\"])\n            bertscore_precision = np.mean(bertscore_result[\"precision\"])\n            bertscore_recall = np.mean(bertscore_result[\"recall\"])\n    except Exception as e:\n        logging.error(f\"Error computing BERTScore: {e}\", exc_info=True)\n        # Check for common issues like OOM on BERTScore model if running on GPU\n        if \"CUDA out of memory\" in str(e):\n            logging.warning(\"BERTScore calculation failed due to OOM. Consider running on CPU or reducing batch size.\")\n        bertscore_f1 = 0.0 # Assign default value on error\n        bertscore_precision = 0.0\n        bertscore_recall = 0.0\n\n    # Combine results\n    result = {\"bleu\": bleu_score}\n    result[\"bertscore_f1\"] = bertscore_f1\n    result[\"bertscore_precision\"] = bertscore_precision\n    result[\"bertscore_recall\"] = bertscore_recall\n\n\n    # Add generation length metric (useful for debugging)\n    try:\n        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n        result[\"gen_len\"] = np.mean(prediction_lens)\n    except Exception as e:\n        logging.warning(f\"Could not compute gen_len: {e}\")\n        result[\"gen_len\"] = 0.0\n\n    return {k: round(v, 4) for k, v in result.items()} # Round metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:01:14.245860Z","iopub.execute_input":"2025-04-14T22:01:14.246189Z","iopub.status.idle":"2025-04-14T22:01:14.267540Z","shell.execute_reply.started":"2025-04-14T22:01:14.246161Z","shell.execute_reply":"2025-04-14T22:01:14.262318Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# --- 6. Training Arguments ---\nlogging.info(\"Configuring training arguments...\")\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"steps\",\n    eval_steps=EVAL_SAVE_STEPS,\n    save_strategy=\"steps\",\n    save_steps=EVAL_SAVE_STEPS,\n    learning_rate=LEARNING_RATE,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE * 2, # Usually can use larger batch for eval\n    weight_decay=WEIGHT_DECAY,\n    num_train_epochs=NUM_EPOCHS,\n    predict_with_generate=True,      # VERY Important for Seq2Seq metrics\n    logging_dir=f\"{OUTPUT_DIR}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=LOGGING_STEPS,\n    load_best_model_at_end=True,     # Load best model based on metric_for_best_model\n    metric_for_best_model=\"bertscore_f1\", # Optimize for BERTScore F1 (often better aligns with semantics)\n    greater_is_better=True,\n    # push_to_hub=False,             # Set to True to upload to Hugging Face Hub\n    fp16=torch.cuda.is_available(),  # Enable mixed precision training if GPU supports it (speeds up, saves memory)\n    # report_to=\"tensorboard\",       # Enable TensorBoard logging\n    generation_max_length=MAX_TARGET_LENGTH, # Ensure generated sequences aren't too long during eval\n    save_total_limit=3,              # Keep only the best 3 checkpoints\n)\n\n# --- 7. Data Collator ---\n# Pads inputs and labels dynamically\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:01:16.533567Z","iopub.execute_input":"2025-04-14T22:01:16.533870Z","iopub.status.idle":"2025-04-14T22:01:22.303132Z","shell.execute_reply.started":"2025-04-14T22:01:16.533844Z","shell.execute_reply":"2025-04-14T22:01:22.296693Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:01:16,536 - INFO - Configuring training arguments...\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1744668078.587483      10 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:232\n2025-04-14 22:01:22,283 - WARNING - torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n2025-04-14 22:01:22,287 - WARNING - torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# --- 8. Trainer ---\nlogging.info(\"Initializing Trainer...\")\n# Add EarlyStopping callback\nearly_stopping = EarlyStoppingCallback(\n    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n    early_stopping_threshold=EARLY_STOPPING_THRESHOLD,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:01:22.336631Z","iopub.execute_input":"2025-04-14T22:01:22.336863Z","iopub.status.idle":"2025-04-14T22:01:22.348249Z","shell.execute_reply.started":"2025-04-14T22:01:22.336841Z","shell.execute_reply":"2025-04-14T22:01:22.343313Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:01:22,337 - INFO - Initializing Trainer...\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stopping] # Add the callback here\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:01:26.038973Z","iopub.execute_input":"2025-04-14T22:01:26.039301Z","iopub.status.idle":"2025-04-14T22:01:26.597208Z","shell.execute_reply.started":"2025-04-14T22:01:26.039258Z","shell.execute_reply":"2025-04-14T22:01:26.591284Z"}},"outputs":[{"name":"stderr","text":"/kaggle/working/tmp/ipykernel_10/529137534.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# --- 9. Train ---\nlogging.info(\"Starting training...\")\ntry:\n    train_result = trainer.train()\n    logging.info(\"Training finished.\")\n\n    # Save training metrics and state\n    trainer.save_model() # Saves the tokenizer too\n    logging.info(f\"Best model saved to {OUTPUT_DIR}\")\n\n    metrics = train_result.metrics\n    metrics[\"train_samples\"] = len(tokenized_datasets[\"train\"])\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n    logging.info(f\"Training metrics and state saved.\")\n\nexcept Exception as e:\n    logging.error(f\"Error during training: {e}\", exc_info=True)\n    # Save potentially partially trained model if needed\n    # trainer.save_model(os.path.join(OUTPUT_DIR, \"interrupted_model\"))\n    # logging.info(\"Attempted to save interrupted model.\")\n    exit()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:01:28.286713Z","iopub.execute_input":"2025-04-14T22:01:28.287013Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 22:01:28,289 - INFO - Starting training...\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  6/252 03:41 < 3:47:03, 0.02 it/s, Epoch 0.08/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# --- 10. Evaluate on Test Set ---\nlogging.info(\"Evaluating on the test set using the best model...\")\ntry:\n    # Ensure the best model is loaded if load_best_model_at_end=True\n    # (Trainer does this automatically before evaluate if load_best_model_at_end=True)\n    test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"], metric_key_prefix=\"test\")\n\n    metrics = test_metrics\n    metrics[\"test_samples\"] = len(tokenized_datasets[\"test\"])\n    trainer.log_metrics(\"test\", metrics)\n    trainer.save_metrics(\"test\", metrics)\n    logging.info(\"Test set evaluation complete.\")\n    logging.info(f\"Test Metrics: {test_metrics}\")\n\nexcept Exception as e:\n    logging.error(f\"Error during test set evaluation: {e}\", exc_info=True)\n    # You might still want to try inference below even if evaluation failed\n\n# --- 11. Example Inference ---\nlogging.info(\"\\n--- Example Inference ---\")\ntry:\n    # Use the trainer's model (which should be the best one)\n    # Or explicitly load if needed:\n    # logging.info(f\"Loading best model from {OUTPUT_DIR} for inference...\")\n    # model = AutoModelForSeq2SeqLM.from_pretrained(OUTPUT_DIR).to(device)\n    # tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n\n    if len(raw_datasets[\"test\"]) > 0:\n        test_example_input = raw_datasets[\"test\"][0][\"input_text\"]\n        test_example_target = raw_datasets[\"test\"][0][\"target_text\"]\n\n        logging.info(f\"Input Text:\\n{test_example_input}\")\n        logging.info(f\"\\nTarget Text:\\n{test_example_target}\")\n\n        # Prepare input for the model\n        input_text_with_prefix = prefix + test_example_input\n        inputs = tokenizer(input_text_with_prefix, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True, padding=True).to(device)\n\n        # Generate output - Experiment with these parameters!\n        output_sequences = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_length=MAX_TARGET_LENGTH + 10, # Allow slightly longer generation than target max\n            num_beams=5,                       # Beam search often improves coherence and relevance (try 3-10)\n            early_stopping=True,               # Stop generation when beams converge\n            # --- Parameters for Novelty/Diversity ---\n            # temperature=0.8,                 # >1.0 more random, <1.0 more focused. Default is 1.0. Try 0.7-0.9?\n            # top_k=50,                        # Sample from top K most likely tokens. Reduces gibberish. (e.g., 50)\n            # top_p=0.95,                      # Nucleus sampling: sample from smallest set whose prob >= p. (e.g., 0.9-0.95)\n            # no_repeat_ngram_size=3,          # Prevent repeating the same 3-grams. Helps reduce repetition.\n            # num_return_sequences=1,          # Generate multiple sequences if needed (e.g., for evaluation or choice)\n        )\n\n        # Decode the generated sequence(s)\n        generated_texts = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n\n        logging.info(\"\\n--- Generated Text ---\")\n        for i, text in enumerate(generated_texts):\n            logging.info(f\"Output {i+1}: {text}\")\n\n    else:\n        logging.warning(\"Test dataset is empty, cannot perform example inference.\")\n\nexcept Exception as e:\n    logging.error(f\"Error during example inference: {e}\", exc_info=True)\n\nlogging.info(\"\\nScript finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:01:05.039225Z","iopub.status.idle":"2025-04-14T22:01:05.039859Z","shell.execute_reply.started":"2025-04-14T22:01:05.039562Z","shell.execute_reply":"2025-04-14T22:01:05.039577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}