{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11366844,"sourceType":"datasetVersion","datasetId":7115124},{"sourceId":11367077,"sourceType":"datasetVersion","datasetId":7115320}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:44:56.937550Z","iopub.execute_input":"2025-04-13T22:44:56.937825Z","iopub.status.idle":"2025-04-13T22:44:56.967268Z","shell.execute_reply.started":"2025-04-13T22:44:56.937797Z","shell.execute_reply":"2025-04-13T22:44:56.966607Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/endsem-eval-nlp/val.arrow\n/kaggle/input/endsem-eval-nlp/test.arrow\n/kaggle/input/endsem-eval-nlp/train.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/val/cache-ed401026e9103c65.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/val/state.json\n/kaggle/input/next-utterance-prediction-nlp-2025/val/dataset_info.json\n/kaggle/input/next-utterance-prediction-nlp-2025/val/data-00000-of-00001.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/val/cache-cc3928cf36c83499.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/val/cache-00884308fd07a1b2.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/test/state.json\n/kaggle/input/next-utterance-prediction-nlp-2025/test/dataset_info.json\n/kaggle/input/next-utterance-prediction-nlp-2025/test/data-00000-of-00001.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/train/state.json\n/kaggle/input/next-utterance-prediction-nlp-2025/train/dataset_info.json\n/kaggle/input/next-utterance-prediction-nlp-2025/train/cache-e2d63a970d5d5c31.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/train/data-00000-of-00001.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/train/cache-ab1a6768ab0ec5c0.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/train/cache-8604605b7b266c7f.arrow\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset \nfrom datasets import load_from_disk\n\n# Load from arrow file directly\ntrain = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/val.arrow\")\ntest = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/test.arrow\")\nval = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/train.arrow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:44:57.744859Z","iopub.execute_input":"2025-04-13T22:44:57.745176Z","iopub.status.idle":"2025-04-13T22:44:58.721255Z","shell.execute_reply.started":"2025-04-13T22:44:57.745154Z","shell.execute_reply":"2025-04-13T22:44:58.720643Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(train)\nprint(val)\nprint(test)\nprint(type(train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:44:58.918361Z","iopub.execute_input":"2025-04-13T22:44:58.919151Z","iopub.status.idle":"2025-04-13T22:44:58.923410Z","shell.execute_reply.started":"2025-04-13T22:44:58.919125Z","shell.execute_reply":"2025-04-13T22:44:58.922711Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_text', 'target_text'],\n    num_rows: 4008\n})\nDataset({\n    features: ['input_text', 'target_text'],\n    num_rows: 576\n})\nDataset({\n    features: ['input_text', 'target_text'],\n    num_rows: 968\n})\n<class 'datasets.arrow_dataset.Dataset'>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"train_df = train.to_pandas()\ntest_df= test.to_pandas()\nval_df = val.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:44:59.744434Z","iopub.execute_input":"2025-04-13T22:44:59.744674Z","iopub.status.idle":"2025-04-13T22:45:00.065691Z","shell.execute_reply.started":"2025-04-13T22:44:59.744657Z","shell.execute_reply":"2025-04-13T22:45:00.065092Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_df.head(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:00.451366Z","iopub.execute_input":"2025-04-13T22:45:00.451944Z","iopub.status.idle":"2025-04-13T22:45:00.474197Z","shell.execute_reply.started":"2025-04-13T22:45:00.451913Z","shell.execute_reply":"2025-04-13T22:45:00.473540Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                          input_text  \\\n0  T: Hi you how to do it today? [SEP] P: Great. ...   \n1  T: Hi you how to do it today? [SEP] P: Great. ...   \n2  T: Hi you how to do it today? [SEP] P: Great. ...   \n3  T: Hi you how to do it today? [SEP] P: Great. ...   \n\n                                         target_text  \n0                 I'm doing well. Thanks for asking.  \n1                             So you're doing great.  \n2  I know your brother brought you in today and h...  \n3  Alright, so you feel like, everything's kind o...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_text</th>\n      <th>target_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>T: Hi you how to do it today? [SEP] P: Great. ...</td>\n      <td>I'm doing well. Thanks for asking.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>T: Hi you how to do it today? [SEP] P: Great. ...</td>\n      <td>So you're doing great.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>T: Hi you how to do it today? [SEP] P: Great. ...</td>\n      <td>I know your brother brought you in today and h...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>T: Hi you how to do it today? [SEP] P: Great. ...</td>\n      <td>Alright, so you feel like, everything's kind o...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"train_df.iloc[0, 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:01.251130Z","iopub.execute_input":"2025-04-13T22:45:01.251874Z","iopub.status.idle":"2025-04-13T22:45:01.256963Z","shell.execute_reply.started":"2025-04-13T22:45:01.251846Z","shell.execute_reply":"2025-04-13T22:45:01.256211Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'T: Hi you how to do it today? [SEP] P: Great. How are you?'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_df.iloc[0 ,  0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:02.026826Z","iopub.execute_input":"2025-04-13T22:45:02.027649Z","iopub.status.idle":"2025-04-13T22:45:02.032736Z","shell.execute_reply.started":"2025-04-13T22:45:02.027619Z","shell.execute_reply":"2025-04-13T22:45:02.031967Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'T: Hi you how to do it today? [SEP] P: Great. How are you?'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_df.iloc[3,  0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:02.432753Z","iopub.execute_input":"2025-04-13T22:45:02.433511Z","iopub.status.idle":"2025-04-13T22:45:02.438274Z","shell.execute_reply.started":"2025-04-13T22:45:02.433487Z","shell.execute_reply":"2025-04-13T22:45:02.437528Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"T: Hi you how to do it today? [SEP] P: Great. How are you? [SEP] T: I'm doing well. Thanks for asking. [SEP] T: So you're doing great. [SEP] P: I'm doing awesome. [SEP] T: I know your brother brought you in today and he had expressed some concerns about your mood. Do you know what that's about? [SEP] P: I, I think he's worrying for no reason. Because I, you know, I for the last like week and a half, I have been following my passion, which is baking. And I didn't realize this before, because I worked in an office. And I just realized that that's not what I want to do with my life. And I know what I want to do with my life now, and I've never known before. So this is an amazing feeling. And what I want to do is I want to start my own baking business. And so what happened was That a couple. Two weeks ago, my nephew had a bake sale. And I've always I've always been like baking, but I've always been like from the box or like from, you know, whatever. So I tried for the first time making brownies from scratch and they were amazing and everyone loved them at the bake sale and they sold out. And my nephew thought I was a hero, basically, I was and so now it made me realize like I could do this for a living and it would make me feel amazing and make me feel like a hero for all these people. And like my college campus would be kind of cool if I could like put up something about like delivering the bakery like the dorms or like big goods to dorms because you know, college students love that type of thing. But so anyway, so I was thinking about long story short, having this business and I would bake all these cookies and I was gonna put up flyers because I don't I'm not like at the point where I can get like an office space but I don't need an office space because I have my apartment and I have everything I need in my apartment to bake everything I need. And I can also have a BJs membership. So Go to BJs. And I can get everything I need in bulk. So like all the flour and butter and sugar and all that stuff. So it's not an issue if I just do it from my apartment. And so I was putting up flyers and trying to promote my business and has really been working on for the last week and a half, and I feel great. And it's amazing. And that's basically it.\""},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:04.022337Z","iopub.execute_input":"2025-04-13T22:45:04.022871Z","iopub.status.idle":"2025-04-13T22:45:04.837138Z","shell.execute_reply.started":"2025-04-13T22:45:04.022844Z","shell.execute_reply":"2025-04-13T22:45:04.836279Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def preprocess(example):\n    full_text = example['input_text'] + tokenizer.eos_token + example['target_text'] + tokenizer.eos_token\n    tokenized = tokenizer(full_text, truncation=True, padding='max_length', max_length=512)\n    return {\n        'input_ids': tokenized['input_ids'],\n        'attention_mask': tokenized['attention_mask'],\n        'labels': tokenized['input_ids']  # For language modeling\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:09.381453Z","iopub.execute_input":"2025-04-13T22:45:09.382064Z","iopub.status.idle":"2025-04-13T22:45:09.386649Z","shell.execute_reply.started":"2025-04-13T22:45:09.382038Z","shell.execute_reply":"2025-04-13T22:45:09.385813Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### We are adding both `input_ids` and `labels` in the output of the preprocessing function, even though they have the same values (i.e., ``tokenized['input_ids'])``. This is because we are doing a next-token prediction task (language modeling). \n\n### In preprocessing, we are combining the input (like a prompt or training part) and the target (like the expected output or test part) into a single sequence. Then, we tokenize this combined sequence. Both input_ids and labels are set to this tokenized sequence. Later during training, the model will use input_ids as input and labels to calculate the loss by predicting the next tokens in the sequence.","metadata":{}},{"cell_type":"code","source":"processed = [preprocess(row) for _, row in train_df.iterrows()]\nval_processed = [preprocess(row) for _, row in val_df.iterrows()]\ntest_processed = [preprocess(row) for _, row in test_df.iterrows()]\ntokenized_dataset = Dataset.from_list(processed)\ntokenized_val_dataset =  Dataset.from_list(val_processed)\ntokenized_test_dataset = Dataset.from_list(test_processed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:10.731413Z","iopub.execute_input":"2025-04-13T22:45:10.732124Z","iopub.status.idle":"2025-04-13T22:45:30.782159Z","shell.execute_reply.started":"2025-04-13T22:45:10.732090Z","shell.execute_reply":"2025-04-13T22:45:30.781525Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(processed[0])\nprint(len(tokenized_dataset))\nprint(model.config)\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:30.783468Z","iopub.execute_input":"2025-04-13T22:45:30.783725Z","iopub.status.idle":"2025-04-13T22:45:30.789541Z","shell.execute_reply.started":"2025-04-13T22:45:30.783697Z","shell.execute_reply":"2025-04-13T22:45:30.788729Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [51, 25, 15902, 345, 703, 284, 466, 340, 1909, 30, 685, 5188, 47, 60, 350, 25, 3878, 13, 1374, 389, 345, 30, 50256, 40, 1101, 1804, 880, 13, 6930, 329, 4737, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [51, 25, 15902, 345, 703, 284, 466, 340, 1909, 30, 685, 5188, 47, 60, 350, 25, 3878, 13, 1374, 389, 345, 30, 50256, 40, 1101, 1804, 880, 13, 6930, 329, 4737, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]}\n4008\nGPT2Config {\n  \"_attn_implementation_autoset\": true,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"conversational\": {\n      \"max_length\": 1000\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results-small\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=5,\n    logging_dir=\"./logs-small\",\n    eval_strategy=\"epoch\",  # no evaluation during training\n    save_strategy=\"no\",        # no checkpoints saved\n    report_to=\"none\"           # disable reporting to tools like WandB\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,  # use the full dataset now\n    eval_dataset=tokenized_val_dataset,\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:39:48.256007Z","iopub.status.idle":"2025-04-13T21:39:48.256269Z","shell.execute_reply.started":"2025-04-13T21:39:48.256154Z","shell.execute_reply":"2025-04-13T21:39:48.256165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## saving the model \nmodel.save_pretrained(\"./trained_model\")\ntokenizer.save_pretrained(\"./trained_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:30.790301Z","iopub.execute_input":"2025-04-13T22:45:30.790569Z","iopub.status.idle":"2025-04-13T22:45:31.624446Z","shell.execute_reply.started":"2025-04-13T22:45:30.790545Z","shell.execute_reply":"2025-04-13T22:45:31.623563Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('./trained_model/tokenizer_config.json',\n './trained_model/special_tokens_map.json',\n './trained_model/vocab.json',\n './trained_model/merges.txt',\n './trained_model/added_tokens.json',\n './trained_model/tokenizer.json')"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"def generate_response(input_text):\n    # Move model to GPU\n    model.to('cuda')  # Ensure the model is on the GPU\n\n    # Tokenize input text\n    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n\n    # Move input_ids to GPU\n    input_ids = input_ids.to('cuda')  # Move input_ids to the same device as the model\n\n    # Generate response from the model\n    output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.9, temperature=0.8)\n\n    # Decode the response\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    # Filter out any unwanted tokens such as [SEP] or 'P:'\n    response = response.replace('[SEP]', '').replace('P:', '').strip()\n\n    return response\n\n# Test with a custom sentence entered by the user\nuser_input = \"Hello, i am sick and tired \"  # Example, change this input to test with different sentences\nresponse = generate_response(user_input)\nprint(f\"Response: {response}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:45:56.056957Z","iopub.execute_input":"2025-04-13T22:45:56.057240Z","iopub.status.idle":"2025-04-13T22:45:57.033292Z","shell.execute_reply.started":"2025-04-13T22:45:56.057220Z","shell.execute_reply":"2025-04-13T22:45:57.032655Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Response: Hello, i am sick and tired Hi, I'm dad.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"eval_results = trainer.evaluate(tokenized_test_dataset)\n\n# Print evaluation results\nprint(\"Evaluation Results on Test Set:\")\nprint(eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:46:05.270461Z","iopub.execute_input":"2025-04-13T22:46:05.271046Z","iopub.status.idle":"2025-04-13T22:46:05.306536Z","shell.execute_reply.started":"2025-04-13T22:46:05.271022Z","shell.execute_reply":"2025-04-13T22:46:05.305674Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1756846051.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Print evaluation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation Results on Test Set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T21:45:53.661062Z","iopub.execute_input":"2025-04-13T21:45:53.661780Z","iopub.status.idle":"2025-04-13T21:45:56.920874Z","shell.execute_reply.started":"2025-04-13T21:45:53.661750Z","shell.execute_reply":"2025-04-13T21:45:56.920068Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"import evaluate\nbleu = evaluate.load(\"bleu\")\nbertscore = evaluate.load(\"bertscore\")\n\ndef evaluate_model(test_dataset):\n    predictions = []\n    references = []\n\n    for example in test_dataset:\n        input_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n\n        inputs = tokenizer(input_text + tokenizer.eos_token, return_tensors='pt', padding=True).to('cuda')\n\n        output = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            do_sample=True,\n            no_repeat_ngram_size=2,\n            top_k=50,\n            top_p=0.9,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        predictions.append(generated_text)\n\n        # ✅ FIXED: decode label ids to text\n        reference = tokenizer.decode(example['labels'], skip_special_tokens=True)\n        references.append(reference)\n\n    # Compute BLEU — needs tokenized references\n    bleu_results = bleu.compute(\n        predictions=predictions,\n        references=[[ref.split()] for ref in references]\n    )\n\n    # Compute BERTScore — raw string references\n    bertscore_results = bertscore.compute(\n        predictions=predictions,\n        references=references,\n        lang='en'\n    )\n\n    print(f\"BLEU Score: {bleu_results}\")\n    print(f\"BERTScore: {bertscore_results}\")\n\n# Run evaluation\nevaluate_model(tokenized_test_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:55:12.373131Z","iopub.execute_input":"2025-04-13T22:55:12.373412Z","iopub.status.idle":"2025-04-13T22:57:50.150373Z","shell.execute_reply.started":"2025-04-13T22:55:12.373392Z","shell.execute_reply":"2025-04-13T22:57:50.149553Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1169a0222bcd4247acf77a7c3840a8bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957a58f2eacf45c5b657d1e47e3cf1bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f49bb311ed34cd896584abbbe9f824c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005047bfb93642c88680298807c67cdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da61f0369fa34c778f2a66c80b3bc324"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ba0d66dd2f646c6909ed56a34a07043"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"BLEU Score: {'bleu': 0.0023545172006527518, 'precisions': [0.3066479553539375, 0.07605098586941476, 0.03749019291266079, 2.5288989932453106e-06], 'brevity_penalty': 0.34336288348406097, 'length_ratio': 0.48333288842402605, 'translation_length': 398333, 'reference_length': 824138}\nBERTScore: {'precision': [0.9513427019119263, 0.9946163892745972, 0.9984627962112427, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9987034797668457, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375048875808716, 0.9834198355674744, 0.9832203984260559, 0.9999998807907104, 0.9954245090484619, 0.9979285001754761, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9810988903045654, 0.9678171277046204, 0.9790961742401123, 0.9999999403953552, 0.9964114427566528, 0.9999998807907104, 0.996764063835144, 0.9987472295761108, 0.9504750967025757, 0.960787296295166, 0.9949044585227966, 0.9974618554115295, 0.9978313446044922, 0.9986050128936768, 0.9991800785064697, 0.9987200498580933, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9972363710403442, 0.9999999403953552, 1.0, 0.999027669429779, 0.9979975819587708, 0.9973506927490234, 1.0, 0.9987186789512634, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9982085227966309, 0.9962377548217773, 1.0, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9750481247901917, 0.9824450612068176, 0.9923095107078552, 0.9952137470245361, 0.9959299564361572, 0.9987253546714783, 0.9983689785003662, 0.9906936287879944, 0.9984238147735596, 0.9988958239555359, 1.0, 1.0, 1.0, 0.8935049176216125, 0.9872636198997498, 0.9901303052902222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9905425310134888, 0.995689868927002, 0.9989520311355591, 0.9984617233276367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743591547012329, 0.97794508934021, 0.9991347789764404, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9795243144035339, 0.9832596778869629, 0.9999999403953552, 0.995949387550354, 0.9965954422950745, 0.9982699155807495, 0.9979673624038696, 0.9983826279640198, 1.0, 0.9970561861991882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9983626008033752, 0.9968792200088501, 0.9999999403953552, 0.9938570261001587, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9558418989181519, 0.9783885478973389, 0.9791985154151917, 0.9700830578804016, 0.996544599533081, 0.9986897706985474, 1.0, 0.9989393949508667, 0.9973455667495728, 1.0, 1.0, 0.9999999403953552, 0.9970420002937317, 0.9976829886436462, 0.9973392486572266, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9850029945373535, 0.9889164566993713, 1.0, 0.997580349445343, 0.9982293248176575, 1.0, 1.0, 0.9999999403953552, 0.9991452693939209, 1.0, 1.0, 1.0, 0.997665524482727, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9804778099060059, 0.9732797145843506, 0.9959921836853027, 0.9945740699768066, 0.9999999403953552, 0.9971359968185425, 0.9986047744750977, 0.9980173110961914, 0.996619462966919, 0.9999999403953552, 0.9999999403953552, 0.9965038895606995, 0.9982700347900391, 0.9986691474914551, 0.9983468055725098, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9842661023139954, 0.9999998807907104, 1.0, 0.9965897798538208, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9552900791168213, 0.9778237342834473, 0.9685471057891846, 0.9820364713668823, 0.9970451593399048, 0.9978360533714294, 0.9972856044769287, 0.9988471269607544, 0.9982913732528687, 0.9933242201805115, 0.9976191520690918, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9577022790908813, 0.9804494380950928, 0.9999999403953552, 0.9978320002555847, 0.998404860496521, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9466917514801025, 0.9903474450111389, 0.9956303834915161, 0.9939204454421997, 0.9981632828712463, 0.9975541830062866, 0.9938578605651855, 0.9991089105606079, 0.9974532723426819, 1.0, 1.0, 0.9537661075592041, 0.996091902256012, 0.9962760210037231, 1.0, 1.0, 1.0, 0.9936440587043762, 1.0, 0.9988024234771729, 0.9990966320037842, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9707343578338623, 0.9658495187759399, 0.9785253405570984, 0.9850149750709534, 0.9954543113708496, 0.9941709637641907, 0.9999999403953552, 0.9990282654762268, 0.9803550243377686, 0.9783293604850769, 0.99365234375, 0.9963059425354004, 0.998710572719574, 0.9999999403953552, 0.987442672252655, 1.0, 1.0, 1.0, 0.9989743232727051, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9615159034729004, 1.0, 0.9983504414558411, 0.9984009265899658, 1.0, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9981909990310669, 1.0, 0.9991143345832825, 1.0, 0.9980958700180054, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9511527419090271, 0.9790449142456055, 0.9908499121665955, 1.0, 1.0, 0.9987760782241821, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9606380462646484, 0.9861606359481812, 0.9904555678367615, 0.9988088607788086, 0.999712347984314, 0.9992654323577881, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9624903202056885, 0.960915207862854, 0.990185022354126, 1.0, 0.9963312149047852, 0.9986107349395752, 0.9981166124343872, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9465228319168091, 0.9841161370277405, 0.9683245420455933, 0.9792020916938782, 0.9936685562133789, 0.9990658760070801, 0.9971722364425659, 0.9985443353652954, 0.9986486434936523, 0.9980990290641785, 0.9979321956634521, 1.0, 1.0, 0.9989034533500671, 0.9990718960762024, 0.9990243315696716, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9726977348327637, 0.9921613931655884, 0.9969921708106995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9973846673965454, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9987843632698059, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9781830310821533, 0.9900899529457092, 0.9999999403953552, 0.9957388639450073, 0.9988567233085632, 0.9969404339790344, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9866868853569031, 0.9999999403953552, 0.9965277910232544, 0.9989566802978516, 0.9999999403953552, 1.0, 1.0, 1.0, 0.9622449278831482, 0.981428861618042, 0.9922641515731812, 0.998361349105835, 0.9961504936218262, 0.996931791305542, 1.0, 1.0, 0.9978773593902588, 0.9982783794403076, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9928978681564331, 0.9985941052436829, 0.997272253036499, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'recall': [0.9915933609008789, 0.9972389340400696, 0.9990617036819458, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9993672370910645, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9800146818161011, 0.9924166202545166, 0.9944512844085693, 0.9999998807907104, 0.9975751638412476, 0.9988273978233337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9908317923545837, 0.9891527891159058, 0.9932093620300293, 0.9999999403953552, 0.9976587295532227, 0.9999998807907104, 0.9989112019538879, 0.9994860291481018, 0.9842126369476318, 0.9910492897033691, 0.9979535937309265, 0.9974644184112549, 0.998319685459137, 0.9992074966430664, 0.9995642900466919, 0.9992192387580872, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9985898733139038, 0.9999999403953552, 1.0, 0.999710738658905, 0.9987698197364807, 0.9982156753540039, 1.0, 0.9991060495376587, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9988350868225098, 0.9973390698432922, 1.0, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9917512536048889, 0.9944499731063843, 0.9978319406509399, 0.9960060119628906, 0.9966429471969604, 0.9985721111297607, 0.9991168975830078, 0.99346923828125, 0.9987953901290894, 0.9993256330490112, 1.0, 1.0, 1.0, 0.9831196069717407, 0.9937418103218079, 0.9950966238975525, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.996442973613739, 0.9965547323226929, 0.9994984269142151, 0.9990009069442749, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9894723892211914, 0.99223792552948, 0.9994703531265259, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9928051233291626, 0.9943597316741943, 0.9999999403953552, 0.9981696605682373, 0.9979490637779236, 0.9988898038864136, 0.9987444877624512, 0.9987980127334595, 1.0, 0.9983423352241516, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9989053606987, 0.9982191324234009, 0.9999999403953552, 0.9961380362510681, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.980629563331604, 0.9884952902793884, 0.9917365312576294, 0.9953052997589111, 0.9976497888565063, 0.9988378882408142, 1.0, 0.999667227268219, 0.9976333975791931, 1.0, 1.0, 0.9999999403953552, 0.9980910420417786, 0.9987837672233582, 0.9984433054924011, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921386241912842, 0.9951890110969543, 1.0, 0.9983134269714355, 0.9993811249732971, 1.0, 1.0, 0.9999999403953552, 0.9995322227478027, 1.0, 1.0, 1.0, 0.9985314607620239, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9919925332069397, 0.9874284863471985, 0.9971243143081665, 0.9965878129005432, 0.9999999403953552, 0.9988521337509155, 0.9994969367980957, 0.9985882043838501, 0.9976198077201843, 0.9999999403953552, 0.9999999403953552, 0.9981454610824585, 0.9987655878067017, 0.9992157220840454, 0.9992106556892395, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9959135055541992, 0.9999998807907104, 1.0, 0.9979339838027954, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9828272461891174, 0.9922136664390564, 0.9938697218894958, 0.9927374124526978, 0.9988235831260681, 0.9989808201789856, 0.998633086681366, 0.9994422197341919, 0.9989961981773376, 0.9981403350830078, 0.9983877539634705, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9880813360214233, 0.9928131699562073, 0.9999999403953552, 0.998993992805481, 0.9991843700408936, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9848835468292236, 0.9969379305839539, 0.9979358911514282, 0.9979875087738037, 0.9988402128219604, 0.9984868764877319, 0.997318685054779, 0.9997215270996094, 0.9986712336540222, 1.0, 1.0, 0.9801994562149048, 0.9975488185882568, 0.9978455305099487, 1.0, 1.0, 1.0, 0.9973052740097046, 1.0, 0.9992180466651917, 0.999485194683075, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9879838824272156, 0.990506649017334, 0.990979790687561, 0.9941601753234863, 0.996827244758606, 0.9966047406196594, 0.9999999403953552, 0.999595582485199, 0.9919620156288147, 0.9938485622406006, 0.9976794719696045, 0.9975272417068481, 0.9993589520454407, 0.9999999403953552, 0.9930787682533264, 1.0, 1.0, 1.0, 0.9993077516555786, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9855350852012634, 1.0, 0.9988812208175659, 0.999306857585907, 1.0, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9992323517799377, 1.0, 0.9994212985038757, 1.0, 0.9989625215530396, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9848721027374268, 0.9895417094230652, 0.9945148825645447, 1.0, 1.0, 0.9992461204528809, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9909859895706177, 0.9935016632080078, 0.9965914487838745, 0.9995457530021667, 0.9997115135192871, 0.9996563196182251, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9856486916542053, 0.9843273162841797, 0.9960049390792847, 1.0, 0.9963265657424927, 0.999123215675354, 0.999009907245636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9834398031234741, 0.9931200742721558, 0.9902317523956299, 0.9920949935913086, 0.9963219165802002, 0.9993941783905029, 0.9980440139770508, 0.9990233182907104, 0.9994878768920898, 0.9984290599822998, 0.9985231757164001, 1.0, 1.0, 0.99936842918396, 0.9996826648712158, 0.9994307160377502, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9930801391601562, 0.9984135627746582, 0.998732328414917, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9981573820114136, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9991253614425659, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9947712421417236, 0.995574951171875, 0.9999999403953552, 0.9983454942703247, 0.9997247457504272, 0.9979265928268433, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9950900077819824, 0.9999999403953552, 0.9986367225646973, 0.9996120929718018, 0.9999999403953552, 1.0, 1.0, 1.0, 0.9882166385650635, 0.9924919605255127, 0.9967243671417236, 0.998536229133606, 0.9973151683807373, 0.9978854060173035, 1.0, 1.0, 0.998551070690155, 0.9988840222358704, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995689332485199, 0.9992728233337402, 0.9987378716468811, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'f1': [0.9710511565208435, 0.9959259033203125, 0.9987621307373047, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9990352392196655, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9582885503768921, 0.9878976941108704, 0.9888039827346802, 0.9999998807907104, 0.9964986443519592, 0.998377799987793, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9859412908554077, 0.9783686399459839, 0.986102283000946, 0.9999999403953552, 0.9970346689224243, 0.9999998807907104, 0.9978364706039429, 0.9991164803504944, 0.9670497179031372, 0.9756836891174316, 0.9964267015457153, 0.9974631667137146, 0.9980754852294922, 0.9989061951637268, 0.999372124671936, 0.998969554901123, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9979126453399658, 0.9999999403953552, 1.0, 0.9993690848350525, 0.9983835220336914, 0.9977830052375793, 1.0, 0.9989123344421387, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9985216856002808, 0.9967881441116333, 1.0, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9833287596702576, 0.9884110689163208, 0.9950631260871887, 0.9956097602844238, 0.9962863326072693, 0.9986487030982971, 0.9987427592277527, 0.9920795559883118, 0.9986096024513245, 0.9991106986999512, 1.0, 1.0, 1.0, 0.9361725449562073, 0.9904921054840088, 0.9926072955131531, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9934840202331543, 0.9961221218109131, 0.9992251396179199, 0.998731255531311, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9818576574325562, 0.9850396513938904, 0.9993025660514832, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9861200451850891, 0.9887785315513611, 0.9999999403953552, 0.9970582723617554, 0.9972717761993408, 0.998579740524292, 0.9983558058738708, 0.9985902905464172, 1.0, 0.9976988434791565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9986339211463928, 0.9975486993789673, 0.9999999403953552, 0.9949962496757507, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9680771231651306, 0.9834159016609192, 0.9854276180267334, 0.9825323224067688, 0.9970968961715698, 0.9987638592720032, 1.0, 0.9993031620979309, 0.9974894523620605, 1.0, 1.0, 0.9999999403953552, 0.997566282749176, 0.9982330799102783, 0.9978908896446228, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9885579347610474, 0.9920428395271301, 1.0, 0.9979467391967773, 0.9988048672676086, 1.0, 1.0, 0.9999999403953552, 0.9993387460708618, 1.0, 1.0, 1.0, 0.9980983138084412, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9862015843391418, 0.9803031086921692, 0.996557891368866, 0.9955799579620361, 0.9999999403953552, 0.9979932904243469, 0.9990506768226624, 0.998302698135376, 0.9971193671226501, 0.9999999403953552, 0.9999999403953552, 0.9973239898681641, 0.9985177516937256, 0.9989423751831055, 0.9987785816192627, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9900556206703186, 0.9999998807907104, 1.0, 0.9972614049911499, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9688630104064941, 0.9849660992622375, 0.9810450077056885, 0.9873579740524292, 0.997933566570282, 0.9984080791473389, 0.9979588389396667, 0.9991446137428284, 0.998643696308136, 0.9957264065742493, 0.9980032444000244, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9726546406745911, 0.9865925908088684, 0.9999999403953552, 0.9984127283096313, 0.9987944960594177, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9654101133346558, 0.9936317801475525, 0.9967817664146423, 0.9959498047828674, 0.9985015988349915, 0.9980202913284302, 0.9955852031707764, 0.9994150996208191, 0.9980618953704834, 1.0, 1.0, 0.9668021202087402, 0.9968198537826538, 0.9970601797103882, 1.0, 1.0, 1.0, 0.9954712390899658, 1.0, 0.9990102648735046, 0.9992908835411072, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9792831540107727, 0.9780226945877075, 0.9847131967544556, 0.989566445350647, 0.9961403012275696, 0.9953863620758057, 0.9999999403953552, 0.9993118643760681, 0.9861243963241577, 0.9860278367996216, 0.9956618547439575, 0.9969161748886108, 0.9990346431732178, 0.9999999403953552, 0.990252673625946, 1.0, 1.0, 1.0, 0.9991410374641418, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9733772873878479, 1.0, 0.9986157417297363, 0.9988536834716797, 1.0, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.998711347579956, 1.0, 0.9992678165435791, 1.0, 0.9985290169715881, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.967718780040741, 0.9842653274536133, 0.9926789999008179, 1.0, 1.0, 0.9990110397338867, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9755761027336121, 0.9898175597190857, 0.9935140609741211, 0.9991771578788757, 0.9997119307518005, 0.9994608163833618, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9739318490028381, 0.9724803566932678, 0.9930864572525024, 1.0, 0.9963288903236389, 0.998866856098175, 0.9985629916191101, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.964628279209137, 0.988597571849823, 0.9791556000709534, 0.9856063723564148, 0.9949935078620911, 0.9992299675941467, 0.997607946395874, 0.9987837672233582, 0.9990680813789368, 0.998263955116272, 0.9982275366783142, 1.0, 1.0, 0.9991358518600464, 0.9993771910667419, 0.9992274641990662, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9827832579612732, 0.9952777028083801, 0.9978615045547485, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9977709054946899, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9989548325538635, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9864073991775513, 0.992824912071228, 0.9999999403953552, 0.9970404505729675, 0.9992905855178833, 0.9974331855773926, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9908705949783325, 0.9999999403953552, 0.9975811243057251, 0.9992842674255371, 0.9999999403953552, 1.0, 1.0, 1.0, 0.97505784034729, 0.9869294166564941, 0.9944892525672913, 0.9984487891197205, 0.9967324733734131, 0.9974083304405212, 1.0, 1.0, 0.998214066028595, 0.9985811114311218, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9942916631698608, 0.9989333748817444, 0.9980045557022095, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.51.1)'}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"pip install bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:47:53.198521Z","iopub.execute_input":"2025-04-13T22:47:53.198779Z","iopub.status.idle":"2025-04-13T22:49:01.514438Z","shell.execute_reply.started":"2025-04-13T22:47:53.198762Z","shell.execute_reply":"2025-04-13T22:49:01.513551Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:47:22.985274Z","iopub.execute_input":"2025-04-13T22:47:22.985803Z","iopub.status.idle":"2025-04-13T22:47:23.402298Z","shell.execute_reply.started":"2025-04-13T22:47:22.985764Z","shell.execute_reply":"2025-04-13T22:47:23.401709Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}