{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11366844,"sourceType":"datasetVersion","datasetId":7115124},{"sourceId":11367077,"sourceType":"datasetVersion","datasetId":7115320}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset \nfrom datasets import load_from_disk\n\n# Load from arrow file directly\ntrain = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/val.arrow\")\ntest = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/test.arrow\")\nval = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/train.arrow\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train)\nprint(val)\nprint(test)\nprint(type(train))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = train.to_pandas()\ntest_df= test.to_pandas()\nval_df = val.to_pandas()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head(4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.iloc[0, 0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.iloc[0 ,  0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.iloc[3,  0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(example):\n    full_text = example['input_text'] + tokenizer.eos_token + example['target_text'] + tokenizer.eos_token\n    tokenized = tokenizer(full_text, truncation=True, padding='max_length', max_length=512)\n    return {\n        'input_ids': tokenized['input_ids'],\n        'attention_mask': tokenized['attention_mask'],\n        'labels': tokenized['input_ids']  # For language modeling\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### We are adding both `input_ids` and `labels` in the output of the preprocessing function, even though they have the same values (i.e., ``tokenized['input_ids'])``. This is because we are doing a next-token prediction task (language modeling). \n\n### In preprocessing, we are combining the input (like a prompt or training part) and the target (like the expected output or test part) into a single sequence. Then, we tokenize this combined sequence. Both input_ids and labels are set to this tokenized sequence. Later during training, the model will use input_ids as input and labels to calculate the loss by predicting the next tokens in the sequence.","metadata":{}},{"cell_type":"code","source":"processed = [preprocess(row) for _, row in train_df.iterrows()]\nval_processed = [preprocess(row) for _, row in val_df.iterrows()]\ntest_processed = [preprocess(row) for _, row in test_df.iterrows()]\ntokenized_dataset = Dataset.from_list(processed)\ntokenized_val_dataset =  Dataset.from_list(val_processed)\ntokenized_test_dataset = Dataset.from_list(test_processed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(processed[0])\nprint(len(tokenized_dataset))\nprint(model.config)\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results-small\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=1,\n    logging_dir=\"./logs-small\",\n    eval_strategy=\"epoch\",  # no evaluation during training\n    save_strategy=\"no\",        # no checkpoints saved\n    report_to=\"none\"           # disable reporting to tools like WandB\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,  # use the full dataset now\n    eval_dataset=tokenized_val_dataset,\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## saving the model \nmodel.save_pretrained(\"./trained_model\")\ntokenizer.save_pretrained(\"./trained_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_response(input_text):\n    # Move model to GPU\n    model.to('cuda')  # Ensure the model is on the GPU\n\n    # Tokenize input text\n    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n\n    # Move input_ids to GPU\n    input_ids = input_ids.to('cuda')  # Move input_ids to the same device as the model\n\n    # Generate response from the model\n    output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.9, temperature=0.8)\n\n    # Decode the response\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    # Filter out any unwanted tokens such as [SEP] or 'P:'\n    response = response.replace('[SEP]', '').replace('P:', '').strip()\n\n    return response\n\n# Test with a custom sentence entered by the user\nuser_input = \"Hello, i am sick and tired \"  # Example, change this input to test with different sentences\nresponse = generate_response(user_input)\nprint(f\"Response: {response}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_results = trainer.evaluate(tokenized_test_dataset)\n\n# Print evaluation results\nprint(\"Evaluation Results on Test Set:\")\nprint(eval_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install evaluate\npip install bert_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\nbleu = evaluate.load(\"bleu\")\nbertscore = evaluate.load(\"bertscore\")\n\ndef evaluate_model(test_dataset):\n    predictions = []\n    references = []\n\n    for example in test_dataset:\n        input_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n\n        inputs = tokenizer(input_text + tokenizer.eos_token, return_tensors='pt', padding=True).to('cuda')\n\n        output = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            do_sample=True,\n            no_repeat_ngram_size=2,\n            top_k=50,\n            top_p=0.9,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        predictions.append(generated_text)\n\n        # ✅ FIXED: decode label ids to text\n        reference = tokenizer.decode(example['labels'], skip_special_tokens=True)\n        references.append(reference)\n\n    # Compute BLEU — needs tokenized references\n    bleu_results = bleu.compute(\n        predictions=predictions,\n        references=[[ref.split()] for ref in references]\n    )\n\n    # Compute BERTScore — raw string references\n    bertscore_results = bertscore.compute(\n        predictions=predictions,\n        references=references,\n        lang='en'\n    )\n\n    print(f\"BLEU Score: {bleu_results}\")\n    print(f\"BERTScore: {bertscore_results}\")\n\n# Run evaluation\nevaluate_model(tokenized_test_dataset)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install bert_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Assuming model.safetensors is in the current working directory\nFileLink(r'/kaggle/working/trained_model.zip')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# Zip the entire directory or just specific files\nshutil.make_archive('/kaggle/working/trained_model', 'zip', './')  # zips everything in current directory\n\n# Display download link\nFileLink('model_files.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:16:21.660472Z","iopub.execute_input":"2025-04-15T00:16:21.660749Z","iopub.status.idle":"2025-04-15T00:16:24.996893Z","shell.execute_reply.started":"2025-04-15T00:16:21.660719Z","shell.execute_reply":"2025-04-15T00:16:24.996106Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import logging\n# Clear existing logging handlers (important in Jupyter)\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)\n\n# Now reconfigure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler()]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:16:24.997890Z","iopub.execute_input":"2025-04-15T00:16:24.998107Z","iopub.status.idle":"2025-04-15T00:16:25.003120Z","shell.execute_reply.started":"2025-04-15T00:16:24.998086Z","shell.execute_reply":"2025-04-15T00:16:25.002307Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport torch\nimport nltk\nimport numpy as np\nfrom datasets import Dataset, DatasetDict, load_from_disk, Dataset as HFDataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM, # *** Changed for DialoGPT ***\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n    # Seq2Seq specific classes removed if not needed elsewhere\n)\nimport evaluate\nimport logging\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:16:31.088773Z","iopub.execute_input":"2025-04-15T00:16:31.089391Z","iopub.status.idle":"2025-04-15T00:16:39.649213Z","shell.execute_reply.started":"2025-04-15T00:16:31.089360Z","shell.execute_reply":"2025-04-15T00:16:39.648630Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 00:16:33,543 - INFO - NumExpr defaulting to 4 threads.\n2025-04-15 00:16:34,089 - INFO - PyTorch version 2.5.1+cu124 available.\n2025-04-15 00:16:34,091 - INFO - Polars version 1.9.0 available.\n2025-04-15 00:16:34,092 - INFO - Duckdb version 1.1.3 available.\n2025-04-15 00:16:34,094 - INFO - TensorFlow version 2.18.0 available.\n2025-04-15 00:16:34,096 - INFO - JAX version 0.4.33 available.\n2025-04-15 00:16:36.812723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744676196.834543     930 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744676196.841236     930 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# --- Basic Setup ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download('punkt', quiet=True)\n\n# --- Configuration ---\n# *** Paths for DialoGPT and Re-ranker ***\n# DIALOGPT_MODEL_DIR = \"./kaggle/working/trained_model\"  # <-- Directory containing your fine-tuned DialoGPT files\nRERANKER_MODEL_NAME = \"bert-base-uncased\"\nRERANKER_OUTPUT_DIR = \"/kaggle/working/reranker_bert_dialogpt\" # <-- Adjusted output dir name\nDATASET_PATH = \"/kaggle/input/endsem-eval-nlp\" # Path to original dataset arrow files\nDIALOGPT_MODEL_DIR = \"/kaggle/working/trained_model\"\n\ndialogpt_tokenizer = AutoTokenizer.from_pretrained(DIALOGPT_MODEL_DIR, local_files_only=True)\ndialogpt_model = AutoModelForCausalLM.from_pretrained(DIALOGPT_MODEL_DIR, local_files_only=True).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:16:48.936194Z","iopub.execute_input":"2025-04-15T00:16:48.936917Z","iopub.status.idle":"2025-04-15T00:16:49.490420Z","shell.execute_reply.started":"2025-04-15T00:16:48.936884Z","shell.execute_reply":"2025-04-15T00:16:49.489854Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# Generation Params for Candidate Generation (DialoGPT)\nNUM_RETURN_SEQUENCES = 5\nNUM_BEAMS = 5\nMAX_HISTORY_LENGTH = 450 # Max tokens for history part (adjust based on DialoGPT limits)\nMAX_RESPONSE_LENGTH = 60 # Max tokens for the generated response part\n\n# Re-ranker Training Params (Same as before)\nRERANKER_MAX_LENGTH = 512 # Ample room for combined text + SEP\nRERANKER_BATCH_SIZE = 16\nRERANKER_EPOCHS = 2\nRERANKER_LR = 3e-5\nlogging.info(\"Loading datasets and fine-tuned DialoGPT model/tokenizer...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:22:51.398616Z","iopub.execute_input":"2025-04-15T00:22:51.399401Z","iopub.status.idle":"2025-04-15T00:22:51.404430Z","shell.execute_reply.started":"2025-04-15T00:22:51.399374Z","shell.execute_reply":"2025-04-15T00:22:51.403771Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 00:22:51,400 - INFO - Loading datasets and fine-tuned DialoGPT model/tokenizer...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\n# --- Load Datasets and DialoGPT Components ---\n\ntry:\n    # Load original datasets\n    val_dataset_orig = Dataset.from_file('/kaggle/input/endsem-eval-nlp/train.arrow')\n    test_dataset_orig = Dataset.from_file('/kaggle/input/endsem-eval-nlp/test.arrow')\n    raw_datasets = DatasetDict({'validation': val_dataset_orig, 'test': test_dataset_orig})\n\n    # *** Load DialoGPT model and tokenizer ***\n    # dialogpt_tokenizer = AutoTokenizer.from_pretrained(DIALOGPT_MODEL_DIR)\n    # dialogpt_model = AutoModelForCausalLM.from_pretrained(DIALOGPT_MODEL_DIR).to(device)\n    dialogpt_model.eval() # Set to evaluation mode\n\n    # *** Set Padding for DialoGPT (GPT2-style) ***\n    if dialogpt_tokenizer.pad_token is None:\n        logging.warning(\"DialoGPT tokenizer has no pad token, setting to eos_token.\")\n        dialogpt_tokenizer.pad_token = dialogpt_tokenizer.eos_token\n    dialogpt_tokenizer.padding_side = \"left\" # Important for decoder-only generation\n\n    # Load BERTScore metric\n    bertscore_metric = evaluate.load(\"bertscore\")\n\nexcept Exception as e:\n    logging.error(f\"Error loading models/datasets: {e}\", exc_info=True)\n    exit()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:17:02.653547Z","iopub.execute_input":"2025-04-15T00:17:02.653846Z","iopub.status.idle":"2025-04-15T00:17:02.967535Z","shell.execute_reply.started":"2025-04-15T00:17:02.653816Z","shell.execute_reply":"2025-04-15T00:17:02.966908Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 2: Generate Candidates using DialoGPT for Validation Set ---\nlogging.info(f\"Generating {NUM_RETURN_SEQUENCES} candidates using DialoGPT for validation set...\")\n\nvalidation_candidates = []\nvalidation_histories_original = [] # Store original format for re-ranker input\nvalidation_targets = []\n\nfor example in tqdm(raw_datasets['validation'], desc=\"Generating Validation Candidates (DialoGPT)\"):\n    history_original = example['input_text'] # Keep original for re-ranker input text\n    target_text = example['target_text']\n    validation_histories_original.append(history_original)\n    validation_targets.append(target_text)\n\n    # Prepare input for DialoGPT: Replace [SEP] with EOS and add final EOS\n    history_formatted = history_original.replace(\"[SEP]\", dialogpt_tokenizer.eos_token) + dialogpt_tokenizer.eos_token\n\n    inputs = dialogpt_tokenizer(\n        history_formatted,\n        return_tensors=\"pt\",\n        max_length=MAX_HISTORY_LENGTH, # Truncate history if too long\n        truncation=True,\n        padding=False # Padding handled later or by generate if needed for batching (but doing one by one here)\n    ).to(device)\n\n    input_ids = inputs['input_ids']\n    current_input_length = input_ids.shape[1]\n\n    # Calculate max_length for generate: current input + desired response length\n    generate_max_length = current_input_length + MAX_RESPONSE_LENGTH\n\n    with torch.no_grad():\n        outputs = dialogpt_model.generate(\n            input_ids,\n            max_length=generate_max_length,\n            num_beams=NUM_BEAMS,\n            num_return_sequences=NUM_RETURN_SEQUENCES,\n            pad_token_id=dialogpt_tokenizer.pad_token_id,\n            eos_token_id=dialogpt_tokenizer.eos_token_id,\n            early_stopping=True,\n            # top_k=50, # Optional sampling parameters if beam search is too slow/rigid\n            # top_p=0.95,\n            # temperature=0.7\n        )\n\n    # *** Decode only the generated part for DialoGPT ***\n    generated_sequences = outputs[:, current_input_length:]\n    decoded_outputs = dialogpt_tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n\n    # Clean up potential artifacts if needed (e.g., dangling EOS)\n    cleaned_outputs = [text.replace(dialogpt_tokenizer.eos_token, \"\").strip() for text in decoded_outputs]\n    validation_candidates.append(cleaned_outputs)\n\nlogging.info(f\"Generated DialoGPT candidates for {len(validation_candidates)} validation examples.\")\n\n# --- Step 3: Prepare Re-ranker Training Data (Identical Logic) ---\nlogging.info(\"Preparing training data for the re-ranker...\")\n\nreranker_train_data = {'text': [], 'label': []}\n\nfor i in tqdm(range(len(validation_histories_original)), desc=\"Scoring DialoGPT Candidates\"):\n    history = validation_histories_original[i] # Use original history format\n    candidates = validation_candidates[i]      # Use DialoGPT candidates\n    target = validation_targets[i]\n\n    # Calculate BERTScore for each candidate against the target\n    try:\n        scores = bertscore_metric.compute(\n            predictions=candidates,\n            references=[target] * len(candidates),\n            lang=\"en\", device=device, batch_size=8 # Adjust batch size if OOM\n        )\n        f1_scores = scores['f1']\n    except Exception as e:\n        logging.warning(f\"BERTScore failed for example {i}. Skipping. Error: {e}\")\n        f1_scores = [0.0] * len(candidates)\n\n    # Create training instances: (history + candidate, score)\n    for j, candidate in enumerate(candidates):\n        # Use original history format for re-ranker input\n        combined_text = history + \" [SEP] \" + candidate # Use [SEP] consistently here\n        reranker_train_data['text'].append(combined_text)\n        reranker_train_data['label'].append(f1_scores[j] if f1_scores and j < len(f1_scores) else 0.0)\n\nreranker_dataset = HFDataset.from_dict(reranker_train_data)\nlogging.info(f\"Re-ranker dataset created with {len(reranker_dataset)} examples (from DialoGPT candidates).\")\n\n# --- Step 4: Train Re-ranker Model (Identical Code) ---\nlogging.info(f\"Loading and training re-ranker model: {RERANKER_MODEL_NAME}\")\n\n# Load re-ranker tokenizer and model (for regression)\nreranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)\nreranker_model = AutoModelForSequenceClassification.from_pretrained(\n    RERANKER_MODEL_NAME,\n    num_labels=1 # Regression\n).to(device)\n\n# Tokenize the re-ranker dataset\ndef tokenize_reranker(examples):\n    tokenized = reranker_tokenizer(\n        examples['text'],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=RERANKER_MAX_LENGTH\n    )\n    tokenized['labels'] = [float(label) for label in examples['label']]\n    return tokenized\n\ntokenized_reranker_dataset = reranker_dataset.map(tokenize_reranker, batched=True, num_proc=2) # Use multiprocessing\ntokenized_reranker_dataset = tokenized_reranker_dataset.remove_columns(['text'])\n\n# Data collator for the re-ranker\nreranker_data_collator = DataCollatorWithPadding(tokenizer=reranker_tokenizer)\n\n# Training arguments for the re-ranker\nreranker_training_args = TrainingArguments(\n    output_dir=RERANKER_OUTPUT_DIR,\n    num_train_epochs=RERANKER_EPOCHS,\n    per_device_train_batch_size=RERANKER_BATCH_SIZE,\n    learning_rate=RERANKER_LR,\n    weight_decay=0.01,\n    logging_dir=f\"{RERANKER_OUTPUT_DIR}/logs\",\n    logging_steps=100,\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\n# Trainer for the re-ranker\nreranker_trainer = Trainer(\n    model=reranker_model,\n    args=reranker_training_args,\n    train_dataset=tokenized_reranker_dataset,\n    tokenizer=reranker_tokenizer,\n    data_collator=reranker_data_collator,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:24:52.911352Z","iopub.execute_input":"2025-04-15T00:24:52.911641Z","iopub.status.idle":"2025-04-15T00:28:44.367473Z","shell.execute_reply.started":"2025-04-15T00:24:52.911621Z","shell.execute_reply":"2025-04-15T00:28:44.366603Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 00:24:52,920 - INFO - Generating 5 candidates using DialoGPT for validation set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Validation Candidates (DialoGPT):   0%|          | 0/576 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1956c63cab94c258a875892a5ed24b3"}},"metadata":{}},{"name":"stderr","text":"2025-04-15 00:28:14,852 - INFO - Generated DialoGPT candidates for 576 validation examples.\n2025-04-15 00:28:14,853 - INFO - Preparing training data for the re-ranker...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Scoring DialoGPT Candidates:   0%|          | 0/576 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734a10bbae0d4e9883f2eea3304e2fd3"}},"metadata":{}},{"name":"stderr","text":"Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n2025-04-15 00:28:37,934 - INFO - Re-ranker dataset created with 2880 examples (from DialoGPT candidates).\n2025-04-15 00:28:37,934 - INFO - Loading and training re-ranker model: bert-base-uncased\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2880 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c45496cbec214aa1a5479bed13e326cc"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_930/774940917.py:130: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  reranker_trainer = Trainer(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"logging.info(\"Starting re-ranker training...\")\nreranker_trainer.train()\nlogging.info(\"Re-ranker training finished.\")\nreranker_trainer.save_model()\nlogging.info(f\"Re-ranker model saved to {RERANKER_OUTPUT_DIR}\")\n\n\n# --- Step 5: Apply Re-ranking on Test Set using DialoGPT Candidates ---\nlogging.info(\"Applying re-ranking to the test set using DialoGPT candidates...\")\n\n# Load the trained re-ranker (Trainer should have loaded the best)\n# Or load manually:\n# reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_OUTPUT_DIR)\n# reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_OUTPUT_DIR).to(device)\nreranker_model.eval()\n\n\nfinal_predictions = []\nground_truths = []\n\nfor example in tqdm(raw_datasets['test'], desc=\"Processing Test Set (DialoGPT + Re-rank)\"):\n    history_original = example['input_text']\n    target = example['target_text']\n    ground_truths.append(target)\n\n    # 1. Generate candidates using DialoGPT (same as validation loop)\n    history_formatted = history_original.replace(\"[SEP]\", dialogpt_tokenizer.eos_token) + dialogpt_tokenizer.eos_token\n    inputs = dialogpt_tokenizer(history_formatted, return_tensors=\"pt\", max_length=MAX_HISTORY_LENGTH, truncation=True).to(device)\n    input_ids = inputs['input_ids']\n    current_input_length = input_ids.shape[1]\n    generate_max_length = current_input_length + MAX_RESPONSE_LENGTH\n\n    with torch.no_grad():\n        outputs = dialogpt_model.generate(\n            input_ids, max_length=generate_max_length, num_beams=NUM_BEAMS,\n            num_return_sequences=NUM_RETURN_SEQUENCES, pad_token_id=dialogpt_tokenizer.pad_token_id,\n            eos_token_id=dialogpt_tokenizer.eos_token_id, early_stopping=True\n        )\n    generated_sequences = outputs[:, current_input_length:]\n    candidates_decoded = dialogpt_tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n    candidates = [text.replace(dialogpt_tokenizer.eos_token, \"\").strip() for text in candidates_decoded]\n\n\n    # 2. Prepare inputs for re-ranker\n    reranker_inputs_text = [history_original + \" [SEP] \" + cand for cand in candidates]\n    reranker_inputs_tokenized = reranker_tokenizer(\n        reranker_inputs_text,\n        return_tensors=\"pt\", truncation=True, padding=True,\n        max_length=RERANKER_MAX_LENGTH\n    ).to(device)\n\n    # 3. Get scores from re-ranker\n    with torch.no_grad():\n        reranker_outputs = reranker_model(**reranker_inputs_tokenized)\n        scores = reranker_outputs.logits.squeeze(-1).cpu().numpy()\n\n    # 4. Select best candidate\n    best_candidate_index = np.argmax(scores)\n    final_prediction = candidates[best_candidate_index]\n    final_predictions.append(final_prediction)\n\nlogging.info(\"Re-ranking complete. Evaluating final DialoGPT + Re-ranked predictions...\")\n\n\n# Remember to compare this to the baseline DialoGPT performance (just using the top beam candidate without re-ranking)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:28:50.972952Z","iopub.execute_input":"2025-04-15T00:28:50.973763Z","iopub.status.idle":"2025-04-15T00:41:59.487087Z","shell.execute_reply.started":"2025-04-15T00:28:50.973730Z","shell.execute_reply":"2025-04-15T00:41:59.485846Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 00:28:50,981 - INFO - Starting re-ranker training...\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [180/180 05:09, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.042800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n2025-04-15 00:34:03,741 - INFO - Re-ranker training finished.\n2025-04-15 00:34:04,781 - INFO - Re-ranker model saved to /kaggle/working/reranker_bert_dialogpt\n2025-04-15 00:34:04,782 - INFO - Applying re-ranking to the test set using DialoGPT candidates...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Test Set (DialoGPT + Re-rank):   0%|          | 0/968 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f8cf58dbcdd4d24933cf278ea66c901"}},"metadata":{}},{"name":"stderr","text":"2025-04-15 00:41:58,222 - INFO - Re-ranking complete. Evaluating final DialoGPT + Re-ranked predictions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a026290f0ca84be5bbbde5060dc7222f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae811014e9f4e22a093b5a9381c0a6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6512a7b69ac64775b879c750a3708627"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_930/2925042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mtokenized_preds_for_bleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoded_preds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mtokenized_labels_for_bleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoded_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mbleu_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_preds_for_bleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_labels_for_bleu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# BERTScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_feature_from_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m_infer_feature_from_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_feature_from_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_infer_feature_from_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m_infer_feature_from_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;34mf\"Input references: {summarize_if_long_list(example['references'])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['so', 'you', \"'re\", ..., 'a', 'job', '?'],\nInput references: [['are', 'you', 'evaluated', 'at', 'work', 'by', 'anybody', 'to', 'see', 'if', 'you', \"'re\", 'in', 'a', 'job', 'you', 'should', 'be', '?']]"],"ename":"ValueError","evalue":"Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['so', 'you', \"'re\", ..., 'a', 'job', '?'],\nInput references: [['are', 'you', 'evaluated', 'at', 'work', 'by', 'anybody', 'to', 'see', 'if', 'you', \"'re\", 'in', 'a', 'job', 'you', 'should', 'be', '?']]","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"# --- Final Evaluation (Identical Code) ---\nbleu_metric = evaluate.load(\"bleu\")\nbertscore_metric = evaluate.load(\"bertscore\") # Reload or ensure it's available\n\n# Post-process for metrics (Keep this part)\ndecoded_preds = [pred.strip() for pred in final_predictions]\ndecoded_labels = [label.strip() for label in ground_truths]\ndecoded_preds = [pred if pred else \"<empty>\" for pred in decoded_preds] # Handle empty predictions\ndecoded_labels = [label if label else \"<empty>\" for label in decoded_labels] # Handle empty labels\n\n# --- CORRECTION FOR BLEU ---\n# BLEU expects lists of strings for predictions, and list of lists of strings for references\n# Do NOT pre-tokenize with nltk here\nlogging.info(\"Calculating BLEU score...\")\nreferences_for_bleu = [[label] for label in decoded_labels] # Each prediction corresponds to a list containing one reference string\ntry:\n    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=references_for_bleu)\n    if bleu_result is None: # Handle potential None return\n        logging.warning(\"BLEU score computation returned None. Setting BLEU to 0.\")\n        bleu_score = 0.0\n    else:\n        bleu_score = bleu_result.get(\"bleu\", 0.0) # Get 'bleu' score, default to 0 if key missing\nexcept Exception as e:\n    logging.error(f\"Error computing BLEU: {e}\", exc_info=True)\n    bleu_score = 0.0 # Set to 0 on error\n# --- END CORRECTION ---\n\n\n# BERTScore (This part expects lists of strings, so it should be correct)\nlogging.info(\"Calculating BERTScore...\")\ntry:\n    bertscore_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\", device=device)\n    if bertscore_result is None or \"f1\" not in bertscore_result:\n        logging.warning(\"BERTScore computation returned None or missing 'f1'. Setting scores to 0.\")\n        bertscore_f1 = 0.0\n        bertscore_precision = 0.0\n        bertscore_recall = 0.0\n    else:\n        bertscore_f1 = np.mean(bertscore_result.get(\"f1\", [0.0])) # Use .get with default\n        bertscore_precision = np.mean(bertscore_result.get(\"precision\", [0.0]))\n        bertscore_recall = np.mean(bertscore_result.get(\"recall\", [0.0]))\nexcept Exception as e:\n    logging.error(f\"Error computing BERTScore: {e}\", exc_info=True)\n    bertscore_f1 = 0.0\n    bertscore_precision = 0.0\n    bertscore_recall = 0.0\n\n\nfinal_metrics = {\n    \"reranked_dialogpt_bleu\": round(bleu_score, 4),\n    \"reranked_dialogpt_bertscore_f1\": round(bertscore_f1, 4),\n    \"reranked_dialogpt_bertscore_precision\": round(bertscore_precision, 4),\n    \"reranked_dialogpt_bertscore_recall\": round(bertscore_recall, 4),\n}\n\nlogging.info(f\"Final Evaluation Metrics (DialoGPT + Re-ranking): {final_metrics}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:45:07.329904Z","iopub.execute_input":"2025-04-15T00:45:07.330465Z","iopub.status.idle":"2025-04-15T00:45:17.681912Z","shell.execute_reply.started":"2025-04-15T00:45:07.330441Z","shell.execute_reply":"2025-04-15T00:45:17.681198Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 00:45:07,914 - INFO - Calculating BLEU score...\n2025-04-15 00:45:08,062 - INFO - Calculating BERTScore...\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2025-04-15 00:45:17,679 - INFO - Final Evaluation Metrics (DialoGPT + Re-ranking): {'reranked_dialogpt_bleu': 0.002, 'reranked_dialogpt_bertscore_f1': 0.842, 'reranked_dialogpt_bertscore_precision': 0.8559, 'reranked_dialogpt_bertscore_recall': 0.83}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}