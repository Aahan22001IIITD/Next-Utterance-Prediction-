{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11366844,"sourceType":"datasetVersion","datasetId":7115124},{"sourceId":11367077,"sourceType":"datasetVersion","datasetId":7115320}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.016508Z","iopub.execute_input":"2025-04-14T23:43:01.016692Z","iopub.status.idle":"2025-04-14T23:43:01.026162Z","shell.execute_reply.started":"2025-04-14T23:43:01.016676Z","shell.execute_reply":"2025-04-14T23:43:01.025463Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/next-utterance-prediction-nlp-2025/val/cache-ed401026e9103c65.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/val/state.json\n/kaggle/input/next-utterance-prediction-nlp-2025/val/dataset_info.json\n/kaggle/input/next-utterance-prediction-nlp-2025/val/data-00000-of-00001.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/val/cache-cc3928cf36c83499.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/val/cache-00884308fd07a1b2.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/test/state.json\n/kaggle/input/next-utterance-prediction-nlp-2025/test/dataset_info.json\n/kaggle/input/next-utterance-prediction-nlp-2025/test/data-00000-of-00001.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/train/state.json\n/kaggle/input/next-utterance-prediction-nlp-2025/train/dataset_info.json\n/kaggle/input/next-utterance-prediction-nlp-2025/train/cache-e2d63a970d5d5c31.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/train/data-00000-of-00001.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/train/cache-ab1a6768ab0ec5c0.arrow\n/kaggle/input/next-utterance-prediction-nlp-2025/train/cache-8604605b7b266c7f.arrow\n/kaggle/input/endsem-eval-nlp/val.arrow\n/kaggle/input/endsem-eval-nlp/test.arrow\n/kaggle/input/endsem-eval-nlp/train.arrow\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset \nfrom datasets import load_from_disk\n\n# Load from arrow file directly\ntrain = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/val.arrow\")\ntest = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/test.arrow\")\nval = Dataset.from_file(\"/kaggle/input/endsem-eval-nlp/train.arrow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.028617Z","iopub.execute_input":"2025-04-14T23:43:01.028813Z","iopub.status.idle":"2025-04-14T23:43:01.040843Z","shell.execute_reply.started":"2025-04-14T23:43:01.028779Z","shell.execute_reply":"2025-04-14T23:43:01.040164Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(train)\nprint(val)\nprint(test)\nprint(type(train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.043887Z","iopub.execute_input":"2025-04-14T23:43:01.044084Z","iopub.status.idle":"2025-04-14T23:43:01.049173Z","shell.execute_reply.started":"2025-04-14T23:43:01.044061Z","shell.execute_reply":"2025-04-14T23:43:01.048446Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_text', 'target_text'],\n    num_rows: 4008\n})\nDataset({\n    features: ['input_text', 'target_text'],\n    num_rows: 576\n})\nDataset({\n    features: ['input_text', 'target_text'],\n    num_rows: 968\n})\n<class 'datasets.arrow_dataset.Dataset'>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"train_df = train.to_pandas()\ntest_df= test.to_pandas()\nval_df = val.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.049838Z","iopub.execute_input":"2025-04-14T23:43:01.050109Z","iopub.status.idle":"2025-04-14T23:43:01.098542Z","shell.execute_reply.started":"2025-04-14T23:43:01.050092Z","shell.execute_reply":"2025-04-14T23:43:01.097842Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_df.head(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.099275Z","iopub.execute_input":"2025-04-14T23:43:01.099490Z","iopub.status.idle":"2025-04-14T23:43:01.106388Z","shell.execute_reply.started":"2025-04-14T23:43:01.099475Z","shell.execute_reply":"2025-04-14T23:43:01.105655Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                          input_text  \\\n0  T: Hi you how to do it today? [SEP] P: Great. ...   \n1  T: Hi you how to do it today? [SEP] P: Great. ...   \n2  T: Hi you how to do it today? [SEP] P: Great. ...   \n3  T: Hi you how to do it today? [SEP] P: Great. ...   \n\n                                         target_text  \n0                 I'm doing well. Thanks for asking.  \n1                             So you're doing great.  \n2  I know your brother brought you in today and h...  \n3  Alright, so you feel like, everything's kind o...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_text</th>\n      <th>target_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>T: Hi you how to do it today? [SEP] P: Great. ...</td>\n      <td>I'm doing well. Thanks for asking.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>T: Hi you how to do it today? [SEP] P: Great. ...</td>\n      <td>So you're doing great.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>T: Hi you how to do it today? [SEP] P: Great. ...</td>\n      <td>I know your brother brought you in today and h...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>T: Hi you how to do it today? [SEP] P: Great. ...</td>\n      <td>Alright, so you feel like, everything's kind o...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"train_df.iloc[0, 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.107147Z","iopub.execute_input":"2025-04-14T23:43:01.107378Z","iopub.status.idle":"2025-04-14T23:43:01.119341Z","shell.execute_reply.started":"2025-04-14T23:43:01.107363Z","shell.execute_reply":"2025-04-14T23:43:01.118664Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'T: Hi you how to do it today? [SEP] P: Great. How are you?'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_df.iloc[0 ,  0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.119986Z","iopub.execute_input":"2025-04-14T23:43:01.120214Z","iopub.status.idle":"2025-04-14T23:43:01.132092Z","shell.execute_reply.started":"2025-04-14T23:43:01.120189Z","shell.execute_reply":"2025-04-14T23:43:01.131499Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'T: Hi you how to do it today? [SEP] P: Great. How are you?'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_df.iloc[3,  0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.132692Z","iopub.execute_input":"2025-04-14T23:43:01.132923Z","iopub.status.idle":"2025-04-14T23:43:01.144365Z","shell.execute_reply.started":"2025-04-14T23:43:01.132909Z","shell.execute_reply":"2025-04-14T23:43:01.143847Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"T: Hi you how to do it today? [SEP] P: Great. How are you? [SEP] T: I'm doing well. Thanks for asking. [SEP] T: So you're doing great. [SEP] P: I'm doing awesome. [SEP] T: I know your brother brought you in today and he had expressed some concerns about your mood. Do you know what that's about? [SEP] P: I, I think he's worrying for no reason. Because I, you know, I for the last like week and a half, I have been following my passion, which is baking. And I didn't realize this before, because I worked in an office. And I just realized that that's not what I want to do with my life. And I know what I want to do with my life now, and I've never known before. So this is an amazing feeling. And what I want to do is I want to start my own baking business. And so what happened was That a couple. Two weeks ago, my nephew had a bake sale. And I've always I've always been like baking, but I've always been like from the box or like from, you know, whatever. So I tried for the first time making brownies from scratch and they were amazing and everyone loved them at the bake sale and they sold out. And my nephew thought I was a hero, basically, I was and so now it made me realize like I could do this for a living and it would make me feel amazing and make me feel like a hero for all these people. And like my college campus would be kind of cool if I could like put up something about like delivering the bakery like the dorms or like big goods to dorms because you know, college students love that type of thing. But so anyway, so I was thinking about long story short, having this business and I would bake all these cookies and I was gonna put up flyers because I don't I'm not like at the point where I can get like an office space but I don't need an office space because I have my apartment and I have everything I need in my apartment to bake everything I need. And I can also have a BJs membership. So Go to BJs. And I can get everything I need in bulk. So like all the flour and butter and sugar and all that stuff. So it's not an issue if I just do it from my apartment. And so I was putting up flyers and trying to promote my business and has really been working on for the last week and a half, and I feel great. And it's amazing. And that's basically it.\""},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:01.145103Z","iopub.execute_input":"2025-04-14T23:43:01.145322Z","iopub.status.idle":"2025-04-14T23:43:28.625617Z","shell.execute_reply.started":"2025-04-14T23:43:01.145307Z","shell.execute_reply":"2025-04-14T23:43:28.624830Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85a5251a9ac34518aff89db34ba6a001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334c27109e544869ba3bc810cebb189b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b1ef7ac812407bb5fd22a8580f9279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7d5efad56f4d0aa91363b233d4eab2"}},"metadata":{}},{"name":"stderr","text":"2025-04-14 23:43:15.824156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744674196.017810      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744674196.074286      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c8c8da601f9421ab5bc0e2af7900cec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40fb63764d3248998cec72314ce004a5"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"def preprocess(example):\n    full_text = example['input_text'] + tokenizer.eos_token + example['target_text'] + tokenizer.eos_token\n    tokenized = tokenizer(full_text, truncation=True, padding='max_length', max_length=512)\n    return {\n        'input_ids': tokenized['input_ids'],\n        'attention_mask': tokenized['attention_mask'],\n        'labels': tokenized['input_ids']  # For language modeling\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:28.626341Z","iopub.execute_input":"2025-04-14T23:43:28.626835Z","iopub.status.idle":"2025-04-14T23:43:28.631444Z","shell.execute_reply.started":"2025-04-14T23:43:28.626787Z","shell.execute_reply":"2025-04-14T23:43:28.630649Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### We are adding both `input_ids` and `labels` in the output of the preprocessing function, even though they have the same values (i.e., ``tokenized['input_ids'])``. This is because we are doing a next-token prediction task (language modeling). \n\n### In preprocessing, we are combining the input (like a prompt or training part) and the target (like the expected output or test part) into a single sequence. Then, we tokenize this combined sequence. Both input_ids and labels are set to this tokenized sequence. Later during training, the model will use input_ids as input and labels to calculate the loss by predicting the next tokens in the sequence.","metadata":{}},{"cell_type":"code","source":"processed = [preprocess(row) for _, row in train_df.iterrows()]\nval_processed = [preprocess(row) for _, row in val_df.iterrows()]\ntest_processed = [preprocess(row) for _, row in test_df.iterrows()]\ntokenized_dataset = Dataset.from_list(processed)\ntokenized_val_dataset =  Dataset.from_list(val_processed)\ntokenized_test_dataset = Dataset.from_list(test_processed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:28.633363Z","iopub.execute_input":"2025-04-14T23:43:28.633616Z","iopub.status.idle":"2025-04-14T23:43:48.054407Z","shell.execute_reply.started":"2025-04-14T23:43:28.633591Z","shell.execute_reply":"2025-04-14T23:43:48.053851Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(processed[0])\nprint(len(tokenized_dataset))\nprint(model.config)\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:43:48.055073Z","iopub.execute_input":"2025-04-14T23:43:48.055325Z","iopub.status.idle":"2025-04-14T23:43:48.061091Z","shell.execute_reply.started":"2025-04-14T23:43:48.055303Z","shell.execute_reply":"2025-04-14T23:43:48.060343Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [51, 25, 15902, 345, 703, 284, 466, 340, 1909, 30, 685, 5188, 47, 60, 350, 25, 3878, 13, 1374, 389, 345, 30, 50256, 40, 1101, 1804, 880, 13, 6930, 329, 4737, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [51, 25, 15902, 345, 703, 284, 466, 340, 1909, 30, 685, 5188, 47, 60, 350, 25, 3878, 13, 1374, 389, 345, 30, 50256, 40, 1101, 1804, 880, 13, 6930, 329, 4737, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]}\n4008\nGPT2Config {\n  \"_attn_implementation_autoset\": true,\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"conversational\": {\n      \"max_length\": 1000\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results-small\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=1,\n    logging_dir=\"./logs-small\",\n    eval_strategy=\"epoch\",  # no evaluation during training\n    save_strategy=\"no\",        # no checkpoints saved\n    report_to=\"none\"           # disable reporting to tools like WandB\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,  # use the full dataset now\n    eval_dataset=tokenized_val_dataset,\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:50:03.614738Z","iopub.execute_input":"2025-04-14T23:50:03.615446Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='187' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 187/1002 01:03 < 04:38, 2.93 it/s, Epoch 0.19/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"## saving the model \nmodel.save_pretrained(\"./trained_model\")\ntokenizer.save_pretrained(\"./trained_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_response(input_text):\n    # Move model to GPU\n    model.to('cuda')  # Ensure the model is on the GPU\n\n    # Tokenize input text\n    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n\n    # Move input_ids to GPU\n    input_ids = input_ids.to('cuda')  # Move input_ids to the same device as the model\n\n    # Generate response from the model\n    output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.9, temperature=0.8)\n\n    # Decode the response\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    # Filter out any unwanted tokens such as [SEP] or 'P:'\n    response = response.replace('[SEP]', '').replace('P:', '').strip()\n\n    return response\n\n# Test with a custom sentence entered by the user\nuser_input = \"Hello, i am sick and tired \"  # Example, change this input to test with different sentences\nresponse = generate_response(user_input)\nprint(f\"Response: {response}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:50:01.560104Z","iopub.status.idle":"2025-04-14T23:50:01.560408Z","shell.execute_reply.started":"2025-04-14T23:50:01.560252Z","shell.execute_reply":"2025-04-14T23:50:01.560267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_results = trainer.evaluate(tokenized_test_dataset)\n\n# Print evaluation results\nprint(\"Evaluation Results on Test Set:\")\nprint(eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:07:53.105554Z","iopub.execute_input":"2025-04-14T23:07:53.106221Z","iopub.status.idle":"2025-04-14T23:08:32.948725Z","shell.execute_reply.started":"2025-04-14T23:07:53.106191Z","shell.execute_reply":"2025-04-14T23:08:32.947985Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Evaluation Results on Test Set:\n{'eval_loss': 4.165743827819824, 'eval_runtime': 39.8335, 'eval_samples_per_second': 24.301, 'eval_steps_per_second': 6.075, 'epoch': 5.0}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"pip install evaluate\npip install bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:08:37.993604Z","iopub.execute_input":"2025-04-14T23:08:37.994292Z","iopub.status.idle":"2025-04-14T23:08:43.897777Z","shell.execute_reply.started":"2025-04-14T23:08:37.994267Z","shell.execute_reply":"2025-04-14T23:08:43.896987Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import evaluate\nbleu = evaluate.load(\"bleu\")\nbertscore = evaluate.load(\"bertscore\")\n\ndef evaluate_model(test_dataset):\n    predictions = []\n    references = []\n\n    for example in test_dataset:\n        input_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n\n        inputs = tokenizer(input_text + tokenizer.eos_token, return_tensors='pt', padding=True).to('cuda')\n\n        output = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            do_sample=True,\n            no_repeat_ngram_size=2,\n            top_k=50,\n            top_p=0.9,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        predictions.append(generated_text)\n\n        # ✅ FIXED: decode label ids to text\n        reference = tokenizer.decode(example['labels'], skip_special_tokens=True)\n        references.append(reference)\n\n    # Compute BLEU — needs tokenized references\n    bleu_results = bleu.compute(\n        predictions=predictions,\n        references=[[ref.split()] for ref in references]\n    )\n\n    # Compute BERTScore — raw string references\n    bertscore_results = bertscore.compute(\n        predictions=predictions,\n        references=references,\n        lang='en'\n    )\n\n    print(f\"BLEU Score: {bleu_results}\")\n    print(f\"BERTScore: {bertscore_results}\")\n\n# Run evaluation\nevaluate_model(tokenized_test_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:10:26.722478Z","iopub.execute_input":"2025-04-14T23:10:26.722699Z","iopub.status.idle":"2025-04-14T23:14:52.694066Z","shell.execute_reply.started":"2025-04-14T23:10:26.722679Z","shell.execute_reply":"2025-04-14T23:14:52.692953Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3a64a0a56c4ad3aa3dcb7869e9cad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62c40663120e4f8da95f4d0861699160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f5df3a79197460ab0b61596033c16bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfae4c2bb2924c1e9c37472789b65d02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c04f3f6bf34b0abcec412300baf626"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcdd2df33ba04c8e96b548f7750e2741"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"BLEU Score: {'bleu': 0.002416618581689708, 'precisions': [0.2997300700054378, 0.07419847478184863, 0.036574801017921925, 2.467003823855927e-06], 'brevity_penalty': 0.3610678654582681, 'length_ratio': 0.49537092088946266, 'translation_length': 408254, 'reference_length': 824138}\nBERTScore: {'precision': [1.0, 0.9627022743225098, 1.0, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9868755340576172, 0.9853189587593079, 0.9850596189498901, 0.993739128112793, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9040141105651855, 0.9213390946388245, 0.9333121180534363, 0.9899694323539734, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9782976508140564, 0.9517185091972351, 0.9885143041610718, 1.0, 0.9999998807907104, 0.9893696308135986, 0.9999998807907104, 0.919927179813385, 0.9892134070396423, 0.9721818566322327, 0.9592976570129395, 1.0, 0.9999999403953552, 0.9791369438171387, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9505575895309448, 0.9999999403953552, 0.9861301183700562, 0.9871559739112854, 1.0, 0.9756563901901245, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9894498586654663, 1.0, 0.9999998807907104, 0.9822097420692444, 0.9845786094665527, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9802866578102112, 0.9724240899085999, 0.9856994152069092, 0.9999998807907104, 1.0, 0.9877706170082092, 0.9960184693336487, 0.9873000979423523, 0.9999998807907104, 0.9999998807907104, 1.0, 1.0, 1.0, 0.8790885210037231, 0.9867607355117798, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9775682687759399, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8766098022460938, 0.9736628532409668, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9801726341247559, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999998807907104, 0.9887449741363525, 0.9888309240341187, 0.9855599999427795, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9948977828025818, 0.9731892347335815, 0.9823061227798462, 0.9999999403953552, 0.9936317205429077, 0.9840872287750244, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8725383281707764, 0.9543513059616089, 0.9747339487075806, 0.9543508291244507, 0.9894108772277832, 0.991849958896637, 0.9637834429740906, 1.0, 0.9650899767875671, 0.9901548624038696, 0.9939357042312622, 0.9785025119781494, 0.9768495559692383, 0.9904240369796753, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9161139726638794, 0.9707822799682617, 1.0, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9426299333572388, 0.9619411826133728, 0.9858165979385376, 1.0, 0.9999999403953552, 0.9887374043464661, 0.975742518901825, 0.9898744225502014, 1.0, 0.9711893796920776, 0.9770843982696533, 0.988842248916626, 0.9892570972442627, 0.9900636672973633, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9008072018623352, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8847026824951172, 0.9391975402832031, 0.967966616153717, 0.9859911799430847, 0.9910057187080383, 0.9999998807907104, 1.0, 1.0, 0.9905651211738586, 0.9793298244476318, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9912413954734802, 0.9701217412948608, 0.993233323097229, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9785177707672119, 0.9415171146392822, 0.9512758851051331, 0.9644613265991211, 0.9868466854095459, 0.9725772142410278, 0.9929704666137695, 0.9999999403953552, 1.0, 1.0, 1.0, 0.9136857986450195, 0.951945424079895, 0.9718949794769287, 0.9807366728782654, 0.9942250847816467, 1.0, 0.9865657687187195, 0.9780570864677429, 0.9906665086746216, 0.9966423511505127, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9679429531097412, 1.0, 0.9698731899261475, 0.9494169354438782, 0.9899057149887085, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 0.9941210746765137, 1.0, 1.0, 0.9868839979171753, 0.9869505167007446, 0.9953826665878296, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8986684083938599, 1.0, 0.9890369176864624, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9633264541625977, 0.978498101234436, 1.0, 0.9783830642700195, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9779520034790039, 0.9999998807907104, 0.966813325881958, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9806787967681885, 0.9943831562995911, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9781647324562073, 1.0, 0.9676916599273682, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9916368722915649, 0.9809819459915161, 1.0, 0.9887857437133789, 0.9620418548583984, 0.9733625650405884, 0.9834732413291931, 1.0, 0.9889459609985352, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 1.0, 0.9999999403953552, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9484924077987671, 0.9773244261741638, 0.9892857074737549, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9967300891876221, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9729068875312805, 0.9999999403953552, 0.9866892099380493, 0.9914613962173462, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 0.9794754385948181, 0.9957347512245178, 1.0, 1.0, 1.0, 1.0, 0.9999998211860657, 0.944866418838501, 0.9953811168670654, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9934044480323792, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9934834241867065, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'recall': [1.0, 0.9957728981971741, 1.0, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9976479411125183, 0.9969635605812073, 0.9979212284088135, 0.9988874793052673, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9759863018989563, 0.9899290204048157, 0.9901670217514038, 0.9974496364593506, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9935954809188843, 0.995288610458374, 0.997956395149231, 1.0, 0.9999998807907104, 0.9982751607894897, 0.9999998807907104, 0.9765559434890747, 0.9967731237411499, 0.9947659969329834, 0.9938620924949646, 1.0, 0.9999999403953552, 0.9979333877563477, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995431661605835, 0.9999999403953552, 0.9969677925109863, 0.998865008354187, 1.0, 0.9971776008605957, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995093047618866, 1.0, 0.9999998807907104, 0.9975382089614868, 0.9963597059249878, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.993396520614624, 0.9959938526153564, 0.9965724945068359, 0.9999998807907104, 1.0, 0.997711181640625, 0.9984399676322937, 0.9972108006477356, 0.9999998807907104, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9736775755882263, 0.996228039264679, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9948204755783081, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9798117876052856, 0.9917634725570679, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9953848123550415, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999998807907104, 0.9976364970207214, 0.9976749420166016, 0.9976763725280762, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9982719421386719, 0.9947282075881958, 0.9974423050880432, 0.9999999403953552, 0.9983415603637695, 0.9955707788467407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9774994254112244, 0.9887311458587646, 0.9930509328842163, 0.9918175935745239, 0.9966707825660706, 0.9963223934173584, 0.9912289381027222, 1.0, 0.9885204434394836, 0.9963241815567017, 0.9978073239326477, 0.997344434261322, 0.9958177804946899, 0.9974488615989685, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9838980436325073, 0.9918256998062134, 1.0, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9885470271110535, 0.9918975830078125, 0.995307445526123, 1.0, 0.9999999403953552, 0.9974256753921509, 0.9917842149734497, 0.9978550672531128, 1.0, 0.996616542339325, 0.9972604513168335, 0.997654914855957, 0.9978244304656982, 0.9987120628356934, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9838045239448547, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9651771783828735, 0.9913280010223389, 0.9909462928771973, 0.9946080446243286, 0.9981814622879028, 0.9999998807907104, 1.0, 1.0, 0.998526930809021, 0.9971706867218018, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9970788359642029, 0.9968537092208862, 0.9983625411987305, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9923754930496216, 0.9872826337814331, 0.9931935667991638, 0.9935840368270874, 0.9964197874069214, 0.9963342547416687, 0.9975565671920776, 0.9999999403953552, 1.0, 1.0, 1.0, 0.9823484420776367, 0.9940894842147827, 0.9977065920829773, 0.9954419136047363, 0.9973973035812378, 1.0, 0.9979377388954163, 0.9974744319915771, 0.9974644184112549, 0.9985742568969727, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9896783828735352, 1.0, 0.9950076937675476, 0.9922312498092651, 0.9946590662002563, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 0.998813271522522, 1.0, 1.0, 0.9943707585334778, 0.9977608919143677, 0.9977542161941528, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9787583351135254, 1.0, 0.9975887537002563, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.996319591999054, 0.9969149827957153, 1.0, 0.9977763891220093, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9956365823745728, 0.9999998807907104, 0.9933364391326904, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9943478107452393, 0.9992839694023132, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9962365031242371, 1.0, 0.9949732422828674, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9952377676963806, 0.9952616095542908, 1.0, 0.9954078793525696, 0.9938551187515259, 0.9925321340560913, 0.9962896108627319, 1.0, 0.9953978061676025, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 1.0, 0.9999999403953552, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9889105558395386, 0.9965540766716003, 0.9974555969238281, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9986487030982971, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9939416646957397, 0.9999999403953552, 0.9964355230331421, 0.9977285265922546, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 0.9972101449966431, 0.9989753365516663, 1.0, 1.0, 1.0, 1.0, 0.9999998211860657, 0.9922271966934204, 0.9972190856933594, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9983502626419067, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9989646673202515, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'f1': [1.0, 0.9789583683013916, 1.0, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9922325015068054, 0.9911070466041565, 0.991448700428009, 0.9963066577911377, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9386225938796997, 0.9544033408164978, 0.960899293422699, 0.9936954379081726, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9858872890472412, 0.9730160236358643, 0.9932129383087158, 1.0, 0.9999998807907104, 0.9938024878501892, 0.9999998807907104, 0.9473960995674133, 0.9929789304733276, 0.983344316482544, 0.9762739539146423, 1.0, 0.9999999403953552, 0.9884458184242249, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9724771976470947, 0.9999999403953552, 0.9915193915367126, 0.9929759502410889, 1.0, 0.9862996339797974, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9922634959220886, 1.0, 0.9999998807907104, 0.9898146390914917, 0.9904341101646423, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.986798107624054, 0.9840678572654724, 0.9911060929298401, 0.9999998807907104, 1.0, 0.9927160739898682, 0.9972277283668518, 0.9922307133674622, 0.9999998807907104, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9239686131477356, 0.9914717674255371, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9861189126968384, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9253422021865845, 0.9826297760009766, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9877201318740845, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999998807907104, 0.9931707978248596, 0.9932332634925842, 0.9915812015533447, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9965819716453552, 0.983840823173523, 0.9898163080215454, 0.9999999403953552, 0.9959810376167297, 0.9897956848144531, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9220413565635681, 0.971237063407898, 0.9838072061538696, 0.9727236032485962, 0.9930275678634644, 0.9940811991691589, 0.9773135185241699, 1.0, 0.976664662361145, 0.9932299256324768, 0.9958677291870117, 0.9878336191177368, 0.9862424731254578, 0.993924081325531, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9487968683242798, 0.9811911582946777, 1.0, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9650426506996155, 0.9766897559165955, 0.9905393123626709, 1.0, 0.9999999403953552, 0.9930624961853027, 0.9836980104446411, 0.9938486814498901, 1.0, 0.9837386608123779, 0.987069308757782, 0.9932290315628052, 0.9935222864151001, 0.9943690896034241, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9404782652854919, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9231895208358765, 0.9645588994026184, 0.9793217182159424, 0.9902808666229248, 0.9945805668830872, 0.9999998807907104, 1.0, 1.0, 0.9945300221443176, 0.9881697297096252, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9941515326499939, 0.9833060503005981, 0.9957913160324097, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9853979349136353, 0.9638569355010986, 0.971782922744751, 0.9788060784339905, 0.9916101098060608, 0.9843124747276306, 0.9952582120895386, 0.9999999403953552, 1.0, 1.0, 1.0, 0.9467738270759583, 0.9725611209869385, 0.9846315979957581, 0.9880345463752747, 0.9958086013793945, 1.0, 0.9922192096710205, 0.9876703023910522, 0.994053840637207, 0.9976073503494263, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.978689968585968, 1.0, 0.9822796583175659, 0.970352053642273, 0.9922767281532288, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 0.9964616298675537, 1.0, 1.0, 0.9906132221221924, 0.9923262596130371, 0.9965670108795166, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9370050430297852, 1.0, 0.9932944774627686, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9999999403953552, 0.9795452952384949, 0.9876207113265991, 1.0, 0.9879845380783081, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9867150783538818, 0.9999998807907104, 0.9798954129219055, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9874659776687622, 0.9968274831771851, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9871179461479187, 1.0, 0.9811428785324097, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9934340715408325, 0.9880702495574951, 1.0, 0.9920858144760132, 0.9776897430419922, 0.982853889465332, 0.9898399710655212, 1.0, 0.9921614527702332, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 1.0, 0.9999999403953552, 0.9999998807907104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9682798981666565, 0.9868456125259399, 0.9933538436889648, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9976884722709656, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999998807907104, 0.9833117723464966, 0.9999999403953552, 0.991538405418396, 0.9945850968360901, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 0.9999999403953552, 1.0, 0.9882633090019226, 0.9973524212837219, 1.0, 1.0, 1.0, 1.0, 0.9999998211860657, 0.9679678678512573, 0.9962992072105408, 0.9999998807907104, 1.0, 1.0, 1.0, 0.9958711862564087, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9962164759635925, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.51.1)'}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"pip install bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:09:09.234523Z","iopub.execute_input":"2025-04-14T23:09:09.234851Z","iopub.status.idle":"2025-04-14T23:10:26.720900Z","shell.execute_reply.started":"2025-04-14T23:09:09.234830Z","shell.execute_reply":"2025-04-14T23:10:26.719976Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T22:47:22.985274Z","iopub.execute_input":"2025-04-13T22:47:22.985803Z","iopub.status.idle":"2025-04-13T22:47:23.402298Z","shell.execute_reply.started":"2025-04-13T22:47:22.985764Z","shell.execute_reply":"2025-04-13T22:47:23.401709Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Assuming model.safetensors is in the current working directory\nFileLink(r'/kaggle/working/trained_model.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:38:13.917748Z","iopub.execute_input":"2025-04-14T23:38:13.918569Z","iopub.status.idle":"2025-04-14T23:38:13.923808Z","shell.execute_reply.started":"2025-04-14T23:38:13.918539Z","shell.execute_reply":"2025-04-14T23:38:13.922923Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/trained_model.zip","text/html":"<a href='/kaggle/working/trained_model.zip' target='_blank'>/kaggle/working/trained_model.zip</a><br>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# Zip the entire directory or just specific files\nshutil.make_archive('/kaggle/working/trained_model', 'zip', './')  # zips everything in current directory\n\n# Display download link\nFileLink('model_files.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:36:45.867969Z","iopub.execute_input":"2025-04-14T23:36:45.868483Z","iopub.status.idle":"2025-04-14T23:37:10.762927Z","shell.execute_reply.started":"2025-04-14T23:36:45.868460Z","shell.execute_reply":"2025-04-14T23:37:10.762109Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/model_files.zip","text/html":"Path (<tt>model_files.zip</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"import os\nimport torch\nimport nltk\nimport numpy as np\nfrom datasets import Dataset, DatasetDict, load_from_disk, Dataset as HFDataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM, # *** Changed for DialoGPT ***\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n    # Seq2Seq specific classes removed if not needed elsewhere\n)\nimport evaluate\nimport logging\nfrom tqdm.auto import tqdm\n\n# --- Basic Setup ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnltk.download('punkt', quiet=True)\n\n# --- Configuration ---\n# *** Paths for DialoGPT and Re-ranker ***\nDIALOGPT_MODEL_DIR = \"./kaggle/working/trained_model\"  # <-- Directory containing your fine-tuned DialoGPT files\nRERANKER_MODEL_NAME = \"bert-base-uncased\"\nRERANKER_OUTPUT_DIR = \"/kaggle/working/reranker_bert_dialogpt\" # <-- Adjusted output dir name\nDATASET_PATH = \"/kaggle/input/endsem-eval-nlp\" # Path to original dataset arrow files\n\n# Generation Params for Candidate Generation (DialoGPT)\nNUM_RETURN_SEQUENCES = 5\nNUM_BEAMS = 5\nMAX_HISTORY_LENGTH = 450 # Max tokens for history part (adjust based on DialoGPT limits)\nMAX_RESPONSE_LENGTH = 60 # Max tokens for the generated response part\n\n# Re-ranker Training Params (Same as before)\nRERANKER_MAX_LENGTH = MAX_HISTORY_LENGTH + MAX_RESPONSE_LENGTH + 20 # Ample room for combined text + SEP\nRERANKER_BATCH_SIZE = 16\nRERANKER_EPOCHS = 2\nRERANKER_LR = 3e-5\n\n# --- Load Datasets and DialoGPT Components ---\nlogging.info(\"Loading datasets and fine-tuned DialoGPT model/tokenizer...\")\ntry:\n    # Load original datasets\n    val_dataset_orig = load_from_disk(os.path.join(DATASET_PATH, \"val.arrow\"))\n    test_dataset_orig = load_from_disk(os.path.join(DATASET_PATH, \"test.arrow\"))\n    raw_datasets = DatasetDict({'validation': val_dataset_orig, 'test': test_dataset_orig})\n\n    # *** Load DialoGPT model and tokenizer ***\n    dialogpt_tokenizer = AutoTokenizer.from_pretrained(DIALOGPT_MODEL_DIR)\n    dialogpt_model = AutoModelForCausalLM.from_pretrained(DIALOGPT_MODEL_DIR).to(device)\n    dialogpt_model.eval() # Set to evaluation mode\n\n    # *** Set Padding for DialoGPT (GPT2-style) ***\n    if dialogpt_tokenizer.pad_token is None:\n        logging.warning(\"DialoGPT tokenizer has no pad token, setting to eos_token.\")\n        dialogpt_tokenizer.pad_token = dialogpt_tokenizer.eos_token\n    dialogpt_tokenizer.padding_side = \"left\" # Important for decoder-only generation\n\n    # Load BERTScore metric\n    bertscore_metric = evaluate.load(\"bertscore\")\n\nexcept Exception as e:\n    logging.error(f\"Error loading models/datasets: {e}\", exc_info=True)\n    exit()\n\n# --- Step 2: Generate Candidates using DialoGPT for Validation Set ---\nlogging.info(f\"Generating {NUM_RETURN_SEQUENCES} candidates using DialoGPT for validation set...\")\n\nvalidation_candidates = []\nvalidation_histories_original = [] # Store original format for re-ranker input\nvalidation_targets = []\n\nfor example in tqdm(raw_datasets['validation'], desc=\"Generating Validation Candidates (DialoGPT)\"):\n    history_original = example['input_text'] # Keep original for re-ranker input text\n    target_text = example['target_text']\n    validation_histories_original.append(history_original)\n    validation_targets.append(target_text)\n\n    # Prepare input for DialoGPT: Replace [SEP] with EOS and add final EOS\n    history_formatted = history_original.replace(\"[SEP]\", dialogpt_tokenizer.eos_token) + dialogpt_tokenizer.eos_token\n\n    inputs = dialogpt_tokenizer(\n        history_formatted,\n        return_tensors=\"pt\",\n        max_length=MAX_HISTORY_LENGTH, # Truncate history if too long\n        truncation=True,\n        padding=False # Padding handled later or by generate if needed for batching (but doing one by one here)\n    ).to(device)\n\n    input_ids = inputs['input_ids']\n    current_input_length = input_ids.shape[1]\n\n    # Calculate max_length for generate: current input + desired response length\n    generate_max_length = current_input_length + MAX_RESPONSE_LENGTH\n\n    with torch.no_grad():\n        outputs = dialogpt_model.generate(\n            input_ids,\n            max_length=generate_max_length,\n            num_beams=NUM_BEAMS,\n            num_return_sequences=NUM_RETURN_SEQUENCES,\n            pad_token_id=dialogpt_tokenizer.pad_token_id,\n            eos_token_id=dialogpt_tokenizer.eos_token_id,\n            early_stopping=True,\n            # top_k=50, # Optional sampling parameters if beam search is too slow/rigid\n            # top_p=0.95,\n            # temperature=0.7\n        )\n\n    # *** Decode only the generated part for DialoGPT ***\n    generated_sequences = outputs[:, current_input_length:]\n    decoded_outputs = dialogpt_tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n\n    # Clean up potential artifacts if needed (e.g., dangling EOS)\n    cleaned_outputs = [text.replace(dialogpt_tokenizer.eos_token, \"\").strip() for text in decoded_outputs]\n    validation_candidates.append(cleaned_outputs)\n\nlogging.info(f\"Generated DialoGPT candidates for {len(validation_candidates)} validation examples.\")\n\n# --- Step 3: Prepare Re-ranker Training Data (Identical Logic) ---\nlogging.info(\"Preparing training data for the re-ranker...\")\n\nreranker_train_data = {'text': [], 'label': []}\n\nfor i in tqdm(range(len(validation_histories_original)), desc=\"Scoring DialoGPT Candidates\"):\n    history = validation_histories_original[i] # Use original history format\n    candidates = validation_candidates[i]      # Use DialoGPT candidates\n    target = validation_targets[i]\n\n    # Calculate BERTScore for each candidate against the target\n    try:\n        scores = bertscore_metric.compute(\n            predictions=candidates,\n            references=[target] * len(candidates),\n            lang=\"en\", device=device, batch_size=8 # Adjust batch size if OOM\n        )\n        f1_scores = scores['f1']\n    except Exception as e:\n        logging.warning(f\"BERTScore failed for example {i}. Skipping. Error: {e}\")\n        f1_scores = [0.0] * len(candidates)\n\n    # Create training instances: (history + candidate, score)\n    for j, candidate in enumerate(candidates):\n        # Use original history format for re-ranker input\n        combined_text = history + \" [SEP] \" + candidate # Use [SEP] consistently here\n        reranker_train_data['text'].append(combined_text)\n        reranker_train_data['label'].append(f1_scores[j] if f1_scores and j < len(f1_scores) else 0.0)\n\nreranker_dataset = HFDataset.from_dict(reranker_train_data)\nlogging.info(f\"Re-ranker dataset created with {len(reranker_dataset)} examples (from DialoGPT candidates).\")\n\n# --- Step 4: Train Re-ranker Model (Identical Code) ---\nlogging.info(f\"Loading and training re-ranker model: {RERANKER_MODEL_NAME}\")\n\n# Load re-ranker tokenizer and model (for regression)\nreranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)\nreranker_model = AutoModelForSequenceClassification.from_pretrained(\n    RERANKER_MODEL_NAME,\n    num_labels=1 # Regression\n).to(device)\n\n# Tokenize the re-ranker dataset\ndef tokenize_reranker(examples):\n    tokenized = reranker_tokenizer(\n        examples['text'],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=RERANKER_MAX_LENGTH\n    )\n    tokenized['labels'] = [float(label) for label in examples['label']]\n    return tokenized\n\ntokenized_reranker_dataset = reranker_dataset.map(tokenize_reranker, batched=True, num_proc=2) # Use multiprocessing\ntokenized_reranker_dataset = tokenized_reranker_dataset.remove_columns(['text'])\n\n# Data collator for the re-ranker\nreranker_data_collator = DataCollatorWithPadding(tokenizer=reranker_tokenizer)\n\n# Training arguments for the re-ranker\nreranker_training_args = TrainingArguments(\n    output_dir=RERANKER_OUTPUT_DIR,\n    num_train_epochs=RERANKER_EPOCHS,\n    per_device_train_batch_size=RERANKER_BATCH_SIZE,\n    learning_rate=RERANKER_LR,\n    weight_decay=0.01,\n    logging_dir=f\"{RERANKER_OUTPUT_DIR}/logs\",\n    logging_steps=100,\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\"\n)\n\n# Trainer for the re-ranker\nreranker_trainer = Trainer(\n    model=reranker_model,\n    args=reranker_training_args,\n    train_dataset=tokenized_reranker_dataset,\n    tokenizer=reranker_tokenizer,\n    data_collator=reranker_data_collator,\n)\n\nlogging.info(\"Starting re-ranker training...\")\nreranker_trainer.train()\nlogging.info(\"Re-ranker training finished.\")\nreranker_trainer.save_model()\nlogging.info(f\"Re-ranker model saved to {RERANKER_OUTPUT_DIR}\")\n\n\n# --- Step 5: Apply Re-ranking on Test Set using DialoGPT Candidates ---\nlogging.info(\"Applying re-ranking to the test set using DialoGPT candidates...\")\n\n# Load the trained re-ranker (Trainer should have loaded the best)\n# Or load manually:\n# reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_OUTPUT_DIR)\n# reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_OUTPUT_DIR).to(device)\nreranker_model.eval()\n\n\nfinal_predictions = []\nground_truths = []\n\nfor example in tqdm(raw_datasets['test'], desc=\"Processing Test Set (DialoGPT + Re-rank)\"):\n    history_original = example['input_text']\n    target = example['target_text']\n    ground_truths.append(target)\n\n    # 1. Generate candidates using DialoGPT (same as validation loop)\n    history_formatted = history_original.replace(\"[SEP]\", dialogpt_tokenizer.eos_token) + dialogpt_tokenizer.eos_token\n    inputs = dialogpt_tokenizer(history_formatted, return_tensors=\"pt\", max_length=MAX_HISTORY_LENGTH, truncation=True).to(device)\n    input_ids = inputs['input_ids']\n    current_input_length = input_ids.shape[1]\n    generate_max_length = current_input_length + MAX_RESPONSE_LENGTH\n\n    with torch.no_grad():\n        outputs = dialogpt_model.generate(\n            input_ids, max_length=generate_max_length, num_beams=NUM_BEAMS,\n            num_return_sequences=NUM_RETURN_SEQUENCES, pad_token_id=dialogpt_tokenizer.pad_token_id,\n            eos_token_id=dialogpt_tokenizer.eos_token_id, early_stopping=True\n        )\n    generated_sequences = outputs[:, current_input_length:]\n    candidates_decoded = dialogpt_tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n    candidates = [text.replace(dialogpt_tokenizer.eos_token, \"\").strip() for text in candidates_decoded]\n\n\n    # 2. Prepare inputs for re-ranker\n    reranker_inputs_text = [history_original + \" [SEP] \" + cand for cand in candidates]\n    reranker_inputs_tokenized = reranker_tokenizer(\n        reranker_inputs_text,\n        return_tensors=\"pt\", truncation=True, padding=True,\n        max_length=RERANKER_MAX_LENGTH\n    ).to(device)\n\n    # 3. Get scores from re-ranker\n    with torch.no_grad():\n        reranker_outputs = reranker_model(**reranker_inputs_tokenized)\n        scores = reranker_outputs.logits.squeeze(-1).cpu().numpy()\n\n    # 4. Select best candidate\n    best_candidate_index = np.argmax(scores)\n    final_prediction = candidates[best_candidate_index]\n    final_predictions.append(final_prediction)\n\nlogging.info(\"Re-ranking complete. Evaluating final DialoGPT + Re-ranked predictions...\")\n\n# --- Final Evaluation (Identical Code) ---\nbleu_metric = evaluate.load(\"bleu\")\nbertscore_metric = evaluate.load(\"bertscore\")\n\n# Post-process for metrics\ndecoded_preds = [pred.strip() for pred in final_predictions]\ndecoded_labels = [label.strip() for label in ground_truths]\ndecoded_preds = [pred if pred else \"<empty>\" for pred in decoded_preds]\ndecoded_labels = [label if label else \"<empty>\" for label in decoded_labels]\n\n# BLEU\ntokenized_preds_for_bleu = [nltk.word_tokenize(pred.lower()) for pred in decoded_preds]\ntokenized_labels_for_bleu = [[nltk.word_tokenize(label.lower())] for label in decoded_labels]\nbleu_result = bleu_metric.compute(predictions=tokenized_preds_for_bleu, references=tokenized_labels_for_bleu)\n\n# BERTScore\nbertscore_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\", device=device)\n\nfinal_metrics = {\n    \"reranked_dialogpt_bleu\": round(bleu_result[\"bleu\"], 4),\n    \"reranked_dialogpt_bertscore_f1\": round(np.mean(bertscore_result[\"f1\"]), 4),\n    \"reranked_dialogpt_bertscore_precision\": round(np.mean(bertscore_result[\"precision\"]), 4),\n    \"reranked_dialogpt_bertscore_recall\": round(np.mean(bertscore_result[\"recall\"]), 4),\n}\n\nlogging.info(f\"Final Evaluation Metrics (DialoGPT + Re-ranking): {final_metrics}\")\n\n# Remember to compare this to the baseline DialoGPT performance (just using the top beam candidate without re-ranking)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T23:50:01.561111Z","iopub.status.idle":"2025-04-14T23:50:01.561323Z","shell.execute_reply.started":"2025-04-14T23:50:01.561223Z","shell.execute_reply":"2025-04-14T23:50:01.561232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}